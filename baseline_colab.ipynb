{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë² ì´ìŠ¤ë¼ì¸\n",
    "\n",
    "## ëŒ€íšŒ ì •ë³´\n",
    "- **Task**: ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ (ê±´ê°•ë³´í—˜ì¦, ì—¬ê¶Œ ë“±)\n",
    "- **Train Data**: ~1,500ì¥ | **Test Data**: ~3,000ì¥\n",
    "- **Metric**: Macro F1 Score | **Framework**: PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ì¶”ì²œ ëª¨ë¸ (ì‹¤í—˜ ìˆœì„œ)\n",
    "\n",
    "### 1ë‹¨ê³„: Baseline â­â­â­â­â­\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b0'\n",
    "```\n",
    "**íŒŒë¼ë¯¸í„°**: 5M | **ì†ë„**: ~1ë¶„/epoch | **ìš©ë„**: ê°€ì¥ ê°€ë³ê³  ë¹ ë¥¸ ì‹œì‘ì \n",
    "\n",
    "### 2ë‹¨ê³„: ì„±ëŠ¥ í–¥ìƒ â­â­â­â­â­\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b1'  # ì¶”ì²œ 1ìˆœìœ„\n",
    "# ë˜ëŠ”\n",
    "CFG.model_name = 'efficientnet_b2'  # ì¶”ì²œ 2ìˆœìœ„\n",
    "```\n",
    "**B1**: 7M, ~1.5ë¶„/epoch, B0 ëŒ€ë¹„ +2~3% í–¥ìƒ  \n",
    "**B2**: 9M, ~2ë¶„/epoch, B0 ëŒ€ë¹„ +3~5% í–¥ìƒ\n",
    "\n",
    "### 3ë‹¨ê³„: ìµœì‹  ì•„í‚¤í…ì²˜ â­â­â­â­â­\n",
    "```python\n",
    "CFG.model_name = 'convnext_tiny'\n",
    "```\n",
    "**íŒŒë¼ë¯¸í„°**: 28M | **ì†ë„**: ~2.5ë¶„/epoch | **íŠ¹ì§•**: ìµœì‹ (2022), B0 ëŒ€ë¹„ +5~7% í–¥ìƒ\n",
    "\n",
    "### 4ë‹¨ê³„: ì„±ëŠ¥ ê·¹ëŒ€í™” â­â­â­â­\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b3'  # 1ìˆœìœ„\n",
    "# ë˜ëŠ”\n",
    "CFG.model_name = 'convnext_small'   # 2ìˆœìœ„ (ìµœê³  ì„±ëŠ¥)\n",
    "```\n",
    "**B3**: 12M, ~2.5ë¶„/epoch, B0 ëŒ€ë¹„ +5~8% í–¥ìƒ  \n",
    "**ConvNeXt-Small**: 50M, ~4ë¶„/epoch, B0 ëŒ€ë¹„ +7~10% í–¥ìƒ\n",
    "\n",
    "### 5ë‹¨ê³„: Transformer (ì„ íƒ) â­â­\n",
    "```python\n",
    "CFG.model_name = 'vit_base_patch16_224'\n",
    "# ë˜ëŠ”\n",
    "CFG.model_name = 'swin_base_patch4_window7_224'\n",
    "```\n",
    "**ViT**: 86M, ~5ë¶„/epoch | **Swin**: 88M, ~5ë¶„/epoch  \n",
    "**ì£¼ì˜**: 1,500ì¥ì—ì„  ê³¼ì í•© ìœ„í—˜ ë†’ìŒ, ê°•í•œ ì¦ê°• í•„ìš”, **ë¹„ì¶”ì²œ**\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ ë¹„ì¶”ì²œ ëª¨ë¸\n",
    "- `resnet50` / `resnet101` - EfficientNetë³´ë‹¤ ë¹„íš¨ìœ¨ì \n",
    "- `mobilenetv3_large_100` - ì†ë„ ë¹ ë¥´ì§€ë§Œ ì„±ëŠ¥ ë‚®ìŒ\n",
    "- `vit` / `swin` - ë°ì´í„° ë¶€ì¡± ì‹œ ê³¼ì í•© (10,000ì¥ ì´ìƒì¼ ë•Œ ì¶”ì²œ)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤\n",
    "\n",
    "### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ ì‹¤í—˜ (2ì‹œê°„)\n",
    "1. B0 + medium ì¦ê°• â†’ 30ë¶„\n",
    "2. B0 + light ì¦ê°• â†’ 30ë¶„\n",
    "3. B0 + heavy ì¦ê°• â†’ 30ë¶„\n",
    "4. B1 + ìµœê³  ì¦ê°• â†’ 40ë¶„\n",
    "\n",
    "### ì‹œë‚˜ë¦¬ì˜¤ 2: ê· í˜• ì‹¤í—˜ (4ì‹œê°„)\n",
    "B0(3ê°€ì§€ ì¦ê°•) â†’ B1 â†’ B2 â†’ ConvNeXt-Tiny â†’ ìµœê³  ëª¨ë¸ ì¬í•™ìŠµ\n",
    "\n",
    "### ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœê³  ì„±ëŠ¥ (í•˜ë£¨)\n",
    "B0 ìµœì í™” â†’ B1/B2 â†’ ConvNeXt-Tiny â†’ B3 â†’ ConvNeXt-Small â†’ ì•™ìƒë¸”\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n",
    "\n",
    "**GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**: `batch_size = 16` ë˜ëŠ” `img_size = 192`  \n",
    "**í•™ìŠµ ëŠë¦¼**: `epochs = 20` (EfficientNet-B0/B1ì€ 20 epoch ì¶©ë¶„)  \n",
    "**ì„±ëŠ¥ plateau**: ëª¨ë¸ í¬ê¸°ë³´ë‹¤ ì¦ê°•/í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¨¼ì €!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install timm wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import wandb\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ (ì„ íƒì‚¬í•­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ (ë°ì´í„°ë‚˜ ëª¨ë¸ì„ ë“œë¼ì´ë¸Œì— ì €ì¥í•˜ë ¤ë©´ ì‹¤í–‰)\n",
    "# ì‹¤í–‰í•˜ë©´ ì¸ì¦ ë§í¬ê°€ ë‚˜íƒ€ë‚˜ê³ , ê¶Œí•œ ìŠ¹ì¸ í›„ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"êµ¬ê¸€ ë“œë¼ì´ë¸Œê°€ /content/drive ì— ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"ë°ì´í„° ê²½ë¡œ ì˜ˆì‹œ: /content/drive/MyDrive/your_data_folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WandB ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB ë¡œê·¸ì¸ (ì²˜ìŒ ì‹¤í–‰ì‹œ API í‚¤ ì…ë ¥ í•„ìš”)\n",
    "# https://wandb.ai/authorize ì—ì„œ API í‚¤ ë°œê¸‰\n",
    "wandb.login()\n",
    "\n",
    "# í”„ë¡œì íŠ¸ëª…ì€ ì‹¤ì œ ëŒ€íšŒëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”\n",
    "WANDB_PROJECT = \"document-classification\"\n",
    "WANDB_ENTITY = None  # íŒ€ ê³„ì • ì‚¬ìš©ì‹œ íŒ€ëª… ì…ë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ë³„ ê¶Œì¥ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ (ë¬¸ì„œ ì´ë¯¸ì§€ ìµœì í™”)\n# ë¬¸ì„œ ì´ë¯¸ì§€ëŠ” í…ìŠ¤íŠ¸ì™€ ì„¸ë°€í•œ ë””í…Œì¼ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ì¼ë°˜ ì´ë¯¸ì§€ë³´ë‹¤ í° ì‚¬ì´ì¦ˆ ì‚¬ìš©\nMODEL_IMG_SIZES = {\n    'efficientnet_b0': 384,   # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'efficientnet_b1': 416,   # ê¸°ë³¸ 240 â†’ 416ìœ¼ë¡œ ì¦ê°€\n    'efficientnet_b2': 448,   # ê¸°ë³¸ 260 â†’ 448ë¡œ ì¦ê°€\n    'efficientnet_b3': 512,   # ê¸°ë³¸ 300 â†’ 512ë¡œ ì¦ê°€\n    'efficientnet_b4': 512,   # ê¸°ë³¸ 380 â†’ 512 ìœ ì§€\n    'convnext_tiny': 384,     # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'convnext_small': 384,    # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'vit_base_patch16_224': 384,  # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'swin_base_patch4_window7_224': 384,  # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n}\n\nclass CFG:\n    # ë°ì´í„° ê²½ë¡œ\n    train_dir = './data/train'  # í•™ìŠµ ì´ë¯¸ì§€ í´ë”\n    test_dir = './data/test'    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë”\n    \n    # ëª¨ë¸ ì„¤ì •\n    model_name = 'efficientnet_b0'  # timm ëª¨ë¸ëª…\n    num_classes = 10  # ì‹¤ì œ í´ë˜ìŠ¤ ê°œìˆ˜ë¡œ ë³€ê²½ í•„ìš”\n    img_size = MODEL_IMG_SIZES.get(model_name, 384)  # ëª¨ë¸ë³„ ê¶Œì¥ ì‚¬ì´ì¦ˆ ìë™ ì ìš© (ë¬¸ì„œ ì´ë¯¸ì§€ìš©)\n    \n    # í•™ìŠµ ì„¤ì •\n    epochs = 30\n    batch_size = 32\n    learning_rate = 1e-4\n    weight_decay = 1e-5\n    \n    # Early Stopping ì„¤ì •\n    early_stopping_patience = 3  # 3 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨\n    early_stopping_min_delta = 0.0001  # F1 ì°¨ì´ 0.01% ë¯¸ë§Œì€ ê°œì„  ì•„ë‹˜\n    \n    # ë°ì´í„° ë¶„í• \n    val_ratio = 0.2\n    \n    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n    save_to_drive = True  # êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥ ì—¬ë¶€\n    drive_model_dir = '/content/drive/MyDrive/document_classification/models'  # ë“œë¼ì´ë¸Œ ì €ì¥ ê²½ë¡œ\n    local_model_path = 'best_model.pth'  # ë¡œì»¬ ì €ì¥ ê²½ë¡œ\n    \n    # WandB ì„¤ì •\n    use_wandb = True\n    wandb_project = WANDB_PROJECT\n    wandb_entity = WANDB_ENTITY\n    experiment_name = None  # Noneì´ë©´ ìë™ìœ¼ë¡œ ë²ˆí˜¸ ë¶€ì—¬\n    \n    # ì‹¤í—˜ëª… ì ‘ë‘ì‚¬ ì„¤ì •\n    # ì˜µì…˜ 1: ëª¨ë¸ëª… ìë™ ì‚¬ìš© (ê¸°ë³¸, Noneìœ¼ë¡œ ë‘ë©´ ìë™)\n    experiment_prefix = None  # Noneì´ë©´ model_name ì‚¬ìš©\n    \n    # ì˜µì…˜ 2: ì»¤ìŠ¤í…€ prefix ì‚¬ìš© (í•„ìš”ì‹œ ì•„ë˜ ì£¼ì„ í•´ì œ)\n    # experiment_prefix = 'baseline'  # baseline_001, baseline_002 ...\n    # experiment_prefix = 'augment'   # augment_001, augment_002 ...\n    \n    # ê¸°íƒ€\n    num_workers = 2\n    seed = 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„°ì…‹ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„° ì¦ê°• (Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµìš© Transform\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG.img_size, CFG.img_size)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© Transform\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG.img_size, CFG.img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "**ì£¼ì˜**: ë°ì´í„° í´ë” êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆìƒ êµ¬ì¡°:\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ class1/\n",
    "â”‚   â”œâ”€â”€ class2/\n",
    "â”‚   â””â”€â”€ ...\n",
    "â””â”€â”€ test/\n",
    "    â”œâ”€â”€ image1.jpg\n",
    "    â”œâ”€â”€ image2.jpg\n",
    "    â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "ë˜ëŠ” CSV íŒŒì¼ì´ ìˆë‹¤ë©´:\n",
    "```python\n",
    "train_df = pd.read_csv('train.csv')\n",
    "# train_df: ['image_path', 'label'] ì»¬ëŸ¼ í¬í•¨\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1: í´ë” êµ¬ì¡°ë¡œë¶€í„° ë°ì´í„° ë¡œë“œ\n",
    "def load_data_from_folders(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "            \n",
    "        for img_name in os.listdir(class_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    return image_paths, labels, class_to_idx\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "if os.path.exists(CFG.train_dir):\n",
    "    train_paths, train_labels, class_to_idx = load_data_from_folders(CFG.train_dir)\n",
    "    print(f\"Total training images: {len(train_paths)}\")\n",
    "    print(f\"Number of classes: {len(class_to_idx)}\")\n",
    "    print(f\"Classes: {class_to_idx}\")\n",
    "    \n",
    "    # CFG.num_classes ì—…ë°ì´íŠ¸\n",
    "    CFG.num_classes = len(class_to_idx)\n",
    "else:\n",
    "    print(f\"Warning: {CFG.train_dir} does not exist!\")\n",
    "    print(\"Please upload your data or modify the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 2: CSV íŒŒì¼ë¡œë¶€í„° ë°ì´í„° ë¡œë“œ (í•„ìš”ì‹œ ì‚¬ìš©)\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# train_paths = train_df['image_path'].tolist()\n",
    "# train_labels = train_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation ë¶„í• \n",
    "if 'train_paths' in locals():\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_paths, train_labels, \n",
    "        test_size=CFG.val_ratio, \n",
    "        random_state=CFG.seed,\n",
    "        stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_paths)}\")\n",
    "    print(f\"Validation size: {len(val_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "if 'train_paths' in locals():\n",
    "    train_dataset = DocumentDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = DocumentDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=CFG.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. WandB Run ì´ˆê¸°í™” (í•™ìŠµ ì‹œì‘ ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_experiment_number(project_name, prefix, entity=None):\n",
    "    \"\"\"WandBì—ì„œ ê¸°ì¡´ ì‹¤í—˜ë“¤ì„ í™•ì¸í•˜ê³  ë‹¤ìŒ ë²ˆí˜¸ë¥¼ ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  run ê°€ì ¸ì˜¤ê¸°\n",
    "        if entity:\n",
    "            runs = api.runs(f\"{entity}/{project_name}\")\n",
    "        else:\n",
    "            runs = api.runs(project_name)\n",
    "        \n",
    "        # prefixë¡œ ì‹œì‘í•˜ëŠ” runë“¤ì˜ ë²ˆí˜¸ ì¶”ì¶œ\n",
    "        numbers = []\n",
    "        for run in runs:\n",
    "            if run.name.startswith(prefix):\n",
    "                try:\n",
    "                    # 'prefix_123' í˜•íƒœì—ì„œ 123 ì¶”ì¶œ\n",
    "                    num = int(run.name.split('_')[-1])\n",
    "                    numbers.append(num)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # ê°€ì¥ í° ë²ˆí˜¸ + 1 ë°˜í™˜\n",
    "        next_num = max(numbers) + 1 if numbers else 1\n",
    "        return next_num\n",
    "    except:\n",
    "        # API ì ‘ê·¼ ì‹¤íŒ¨ì‹œ 001ë¶€í„° ì‹œì‘\n",
    "        return 1\n",
    "\n",
    "# WandB Run ì´ˆê¸°í™”\n",
    "if CFG.use_wandb:\n",
    "    # experiment_prefixê°€ Noneì´ë©´ ëª¨ë¸ëª… ì‚¬ìš© (ìë™)\n",
    "    if CFG.experiment_prefix is None:\n",
    "        actual_prefix = CFG.model_name\n",
    "    else:\n",
    "        actual_prefix = CFG.experiment_prefix\n",
    "    \n",
    "    # ì‹¤í—˜ëª… ìë™ ìƒì„±\n",
    "    if CFG.experiment_name is None:\n",
    "        exp_num = get_next_experiment_number(\n",
    "            CFG.wandb_project, \n",
    "            actual_prefix,\n",
    "            CFG.wandb_entity\n",
    "        )\n",
    "        CFG.experiment_name = f\"{actual_prefix}_{exp_num:03d}\"\n",
    "    \n",
    "    run = wandb.init(\n",
    "        project=CFG.wandb_project,\n",
    "        entity=CFG.wandb_entity,\n",
    "        name=CFG.experiment_name,\n",
    "        config={\n",
    "            \"model_name\": CFG.model_name,\n",
    "            \"num_classes\": CFG.num_classes,\n",
    "            \"img_size\": CFG.img_size,\n",
    "            \"epochs\": CFG.epochs,\n",
    "            \"batch_size\": CFG.batch_size,\n",
    "            \"learning_rate\": CFG.learning_rate,\n",
    "            \"weight_decay\": CFG.weight_decay,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"scheduler\": \"CosineAnnealingLR\",\n",
    "            \"val_ratio\": CFG.val_ratio,\n",
    "            \"seed\": CFG.seed,\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"WandB Run initialized: {run.name}\")\n",
    "    print(f\"WandB URL: {run.url}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"WandB is disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=True):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        \n",
    "        # ëª¨ë¸ì˜ classifier ë¶€ë¶„ ìˆ˜ì •\n",
    "        if 'efficientnet' in model_name:\n",
    "            in_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(in_features, num_classes)\n",
    "        elif 'resnet' in model_name:\n",
    "            in_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(in_features, num_classes)\n",
    "        elif 'vit' in model_name:\n",
    "            in_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = DocumentClassifier(\n",
    "    model_name=CFG.model_name, \n",
    "    num_classes=CFG.num_classes, \n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: {CFG.model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# WandBì— ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¡œê¹…\n",
    "if CFG.use_wandb:\n",
    "    wandb.watch(model, log='all', log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        batch_loss = running_loss / (batch_idx + 1)\n",
    "        batch_acc = 100. * correct / total\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': batch_loss,\n",
    "            'acc': batch_acc\n",
    "        })\n",
    "        \n",
    "        # WandB ë¡œê¹… (ë§¤ ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if CFG.use_wandb:\n",
    "            wandb.log({\n",
    "                'train/batch_loss': loss.item(),\n",
    "                'train/batch_acc': 100. * predicted.eq(labels).sum().item() / labels.size(0),\n",
    "                'train/step': epoch * len(train_loader) + batch_idx\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    # Macro F1 Score ê³„ì‚°\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ F1 Score ê³„ì‚°\n",
    "    per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
    "    \n",
    "    # Confusion Matrix ê³„ì‚°\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, macro_f1, per_class_f1, cm, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "best_f1 = 0.0\npatience_counter = 0  # Early Stopping ì¹´ìš´í„°\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_f1': []\n}\n\n# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì €ì¥ ê²½ë¡œ ìƒì„±\nif CFG.save_to_drive:\n    os.makedirs(CFG.drive_model_dir, exist_ok=True)\n    print(f\"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {CFG.drive_model_dir}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Early Stopping: Patience={CFG.early_stopping_patience}, Min Delta={CFG.early_stopping_min_delta}\")\nprint(f\"{'='*60}\\n\")\n\nfor epoch in range(CFG.epochs):\n    print(f\"\\nEpoch {epoch+1}/{CFG.epochs}\")\n    print(\"-\" * 50)\n    \n    # í•™ìŠµ\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n    \n    # ê²€ì¦\n    val_loss, val_f1, per_class_f1, cm, val_preds, val_labels = validate(model, val_loader, criterion, device)\n    \n    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # ê²°ê³¼ ì €ì¥\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Macro F1: {val_f1:.4f}\")\n    print(f\"Learning Rate: {current_lr:.6f}\")\n    \n    # WandB ë¡œê¹… (ì—í­ë³„)\n    if CFG.use_wandb:\n        # ê¸°ë³¸ ë©”íŠ¸ë¦­\n        log_dict = {\n            'epoch': epoch + 1,\n            'train/epoch_loss': train_loss,\n            'train/epoch_acc': train_acc,\n            'val/loss': val_loss,\n            'val/macro_f1': val_f1,\n            'learning_rate': current_lr,\n            'early_stopping/patience_counter': patience_counter,\n        }\n        \n        # í´ë˜ìŠ¤ë³„ F1 Score\n        if 'class_to_idx' in locals():\n            idx_to_class = {v: k for k, v in class_to_idx.items()}\n            for idx, f1 in enumerate(per_class_f1):\n                class_name = idx_to_class.get(idx, f'class_{idx}')\n                log_dict[f'val/f1_{class_name}'] = f1\n        \n        # Confusion Matrix (5 ì—í­ë§ˆë‹¤)\n        if (epoch + 1) % 5 == 0:\n            log_dict['val/confusion_matrix'] = wandb.plot.confusion_matrix(\n                probs=None,\n                y_true=val_labels,\n                preds=val_preds,\n                class_names=[idx_to_class.get(i, f'class_{i}') for i in range(CFG.num_classes)] if 'idx_to_class' in locals() else None\n            )\n        \n        wandb.log(log_dict)\n    \n    # Early Stopping ì²´í¬ ë° ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n    if val_f1 > best_f1 + CFG.early_stopping_min_delta:\n        # ì„±ëŠ¥ ê°œì„ ë¨\n        best_f1 = val_f1\n        patience_counter = 0  # ì¹´ìš´í„° ë¦¬ì…‹\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_f1': best_f1,\n        }\n        \n        # ë¡œì»¬ì— ì €ì¥\n        torch.save(checkpoint, CFG.local_model_path)\n        print(f\"âœ“ Best model saved locally! (F1: {best_f1:.4f})\")\n        \n        # êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥\n        if CFG.save_to_drive:\n            # ì‹¤í—˜ëª…ì„ íŒŒì¼ëª…ì— í¬í•¨\n            if CFG.use_wandb and CFG.experiment_name:\n                drive_model_path = f\"{CFG.drive_model_dir}/{CFG.experiment_name}_f1_{best_f1:.4f}.pth\"\n            else:\n                drive_model_path = f\"{CFG.drive_model_dir}/best_model_f1_{best_f1:.4f}.pth\"\n            \n            torch.save(checkpoint, drive_model_path)\n            print(f\"âœ“ Best model saved to drive: {drive_model_path}\")\n        \n        # WandBì— ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n        if CFG.use_wandb:\n            artifact = wandb.Artifact(\n                name=f'model-{run.id}',\n                type='model',\n                description=f'Best model with F1: {best_f1:.4f}',\n                metadata={\n                    'epoch': epoch + 1,\n                    'val_f1': val_f1,\n                    'val_loss': val_loss,\n                }\n            )\n            artifact.add_file(CFG.local_model_path)\n            wandb.log_artifact(artifact)\n    else:\n        # ì„±ëŠ¥ ê°œì„  ì—†ìŒ\n        patience_counter += 1\n        print(f\"âš  No improvement. Patience: {patience_counter}/{CFG.early_stopping_patience}\")\n        \n        # Early Stopping ì²´í¬\n        if patience_counter >= CFG.early_stopping_patience:\n            print(f\"\\n{'='*60}\")\n            print(f\"Early Stopping triggered at epoch {epoch+1}\")\n            print(f\"Best Validation Macro F1: {best_f1:.4f}\")\n            print(f\"{'='*60}\")\n            break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training completed!\")\nprint(f\"Best Validation Macro F1: {best_f1:.4f}\")\nprint(f\"Total epochs: {epoch+1}\")\nif patience_counter >= CFG.early_stopping_patience:\n    print(f\"Stopped early due to no improvement for {CFG.early_stopping_patience} epochs\")\nprint(f\"{'='*50}\")\n\n# ìµœì¢… ëª¨ë¸ ê²½ë¡œ ì¶œë ¥\nprint(f\"\\nëª¨ë¸ ì €ì¥ ìœ„ì¹˜:\")\nprint(f\"  - ë¡œì»¬: {CFG.local_model_path}\")\nif CFG.save_to_drive:\n    print(f\"  - ë“œë¼ì´ë¸Œ: {CFG.drive_model_dir}/\")\n\n# WandB Run ì¢…ë£Œ\nif CFG.use_wandb:\n    wandb.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss ê·¸ë˜í”„\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 Score ê·¸ë˜í”„\n",
    "axes[1].plot(history['val_f1'], label='Val Macro F1', color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Macro F1 Score')\n",
    "axes[1].set_title('Validation Macro F1 Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Best model loaded (F1: {checkpoint['best_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "def load_test_data(test_dir):\n",
    "    test_paths = []\n",
    "    for img_name in sorted(os.listdir(test_dir)):\n",
    "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            test_paths.append(os.path.join(test_dir, img_name))\n",
    "    return test_paths\n",
    "\n",
    "if os.path.exists(CFG.test_dir):\n",
    "    test_paths = load_test_data(CFG.test_dir)\n",
    "    print(f\"Total test images: {len(test_paths)}\")\n",
    "else:\n",
    "    print(f\"Warning: {CFG.test_dir} does not exist!\")\n",
    "    test_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "if test_paths:\n",
    "    test_dataset = DocumentDataset(test_paths, labels=None, transform=val_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  í•¨ìˆ˜\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader, desc='Predicting'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ì¶”ë¡  ì‹¤í–‰\n",
    "if test_paths:\n",
    "    predictions = predict(model, test_loader, device)\n",
    "    print(f\"Prediction completed: {len(predictions)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„± (í˜•ì‹ì€ ëŒ€íšŒ ê·œì •ì— ë§ê²Œ ìˆ˜ì •)\n",
    "if test_paths and predictions:\n",
    "    # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'image': [os.path.basename(path) for path in test_paths],\n",
    "        'label': [idx_to_class[pred] for pred in predictions]\n",
    "    })\n",
    "    \n",
    "    # ë˜ëŠ” ìˆ«ì ë ˆì´ë¸”ë¡œ ì œì¶œí•˜ëŠ” ê²½ìš°:\n",
    "    # submission = pd.DataFrame({\n",
    "    #     'image': [os.path.basename(path) for path in test_paths],\n",
    "    #     'label': predictions\n",
    "    # })\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nSubmission file saved: submission.csv\")\n",
    "    print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´\n",
    "\n",
    "### ëª¨ë¸ ê°œì„ \n",
    "1. **ë‹¤ì–‘í•œ ëª¨ë¸ ì‹œë„**\n",
    "   - `efficientnet_b3`, `efficientnet_b4` (ë” í° ëª¨ë¸)\n",
    "   - `convnext_tiny`, `convnext_small`\n",
    "   - `vit_base_patch16_224` (Vision Transformer)\n",
    "   - `swin_base_patch4_window7_224`\n",
    "\n",
    "2. **ì•™ìƒë¸”**\n",
    "   - ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•© (Voting, Averaging)\n",
    "   - K-Fold Cross Validation\n",
    "\n",
    "3. **ë°ì´í„° ì¦ê°• ê°•í™”**\n",
    "   - AutoAugment, RandAugment\n",
    "   - MixUp, CutMix\n",
    "   - Test Time Augmentation (TTA)\n",
    "\n",
    "4. **í•™ìŠµ ê¸°ë²•**\n",
    "   - Label Smoothing\n",
    "   - Focal Loss (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œ)\n",
    "   - Stochastic Weight Averaging (SWA)\n",
    "   - Gradient Accumulation (ë°°ì¹˜ í¬ê¸° í™•ì¥)\n",
    "\n",
    "5. **ì´ë¯¸ì§€ ì „ì²˜ë¦¬**\n",
    "   - ë¬¸ì„œ ì •ë ¬ (Document alignment)\n",
    "   - í•´ìƒë„ ì¡°ì •\n",
    "   - ë…¸ì´ì¦ˆ ì œê±°\n",
    "\n",
    "### WandB í™œìš© íŒ\n",
    "1. **Sweepì„ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**\n",
    "   - ìë™ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸°\n",
    "   - Bayesian Optimization, Random Search ë“± ì§€ì›\n",
    "\n",
    "2. **ì‹¤í—˜ ë¹„êµ**\n",
    "   - ì—¬ëŸ¬ ì‹¤í—˜ì„ í•œëˆˆì— ë¹„êµ\n",
    "   - Parallel coordinates plotìœ¼ë¡œ ê´€ê³„ ë¶„ì„\n",
    "\n",
    "3. **íŒ€ í˜‘ì—…**\n",
    "   - íŒ€ì›ê³¼ ì‹¤í—˜ ê²°ê³¼ ê³µìœ \n",
    "   - ì½”ë©˜íŠ¸ ë° ë…¸íŠ¸ ê¸°ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì½”ë© í™˜ê²½ íŒ\n",
    "\n",
    "### 1. GPU ëŸ°íƒ€ì„ ì‚¬ìš©\n",
    "ìƒë‹¨ ë©”ë‰´: `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½` â†’ `í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: GPU`\n",
    "\n",
    "### 2. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "### 3. ë°ì´í„° ì—…ë¡œë“œ\n",
    "- ì¢Œì¸¡ íŒŒì¼ íƒ­ì—ì„œ ì—…ë¡œë“œ\n",
    "- ë˜ëŠ” êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ì½ê¸°\n",
    "- Kaggle API ì‚¬ìš© (Kaggle ëŒ€íšŒì¸ ê²½ìš°)\n",
    "\n",
    "### 4. ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥\n",
    "```python\n",
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/models/best_model.pth')\n",
    "```\n",
    "\n",
    "### 5. WandB ì‹¤í—˜ëª… ìë™ ë²ˆí˜¸ ë¶€ì—¬ ì‚¬ìš©ë²• (ì¤‘ìš”!)\n",
    "\n",
    "#### ğŸ“Œ ê¸°ë³¸ ê°œë…\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **ì‹¤í—˜ëª…ì„ ìë™ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ì„œ ê´€ë¦¬**í•˜ëŠ” ê¸°ëŠ¥ì´ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "**ê¸°ë³¸ ì„¤ì •: ëª¨ë¸ëª…ë§Œ ë°”ê¾¸ë©´ ìë™ìœ¼ë¡œ ì‹¤í—˜ëª…ë„ ë³€ê²½ë©ë‹ˆë‹¤!** â­\n",
    "\n",
    "#### âœ¨ ì‚¬ìš©ë²• (ë§¤ìš° ê°„ë‹¨!)\n",
    "\n",
    "**ëª¨ë¸ë§Œ ë°”ê¾¸ë©´ ë!**\n",
    "```python\n",
    "# ì„¹ì…˜ 4ì˜ CFG í´ë˜ìŠ¤ì—ì„œ\n",
    "CFG.model_name = 'efficientnet_b0'  # â† ì´ê²ƒë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤!\n",
    "```\n",
    "â†’ ìë™ìœ¼ë¡œ `efficientnet_b0_001`, `efficientnet_b0_002` ...\n",
    "\n",
    "**ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹¤í—˜í•˜ê³  ì‹¶ë‹¤ë©´?**\n",
    "```python\n",
    "CFG.model_name = 'resnet50'  # â† ëª¨ë¸ëª…ë§Œ ë³€ê²½\n",
    "```\n",
    "â†’ ìë™ìœ¼ë¡œ `resnet50_001`, `resnet50_002` ...\n",
    "\n",
    "ê·¸ê²Œ ë‹¤ì…ë‹ˆë‹¤! ë‚˜ë¨¸ì§€ëŠ” ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤. ğŸ˜Š\n",
    "\n",
    "#### ğŸ”§ ìƒì„¸ ì„¤ì • (ì„ íƒì‚¬í•­)\n",
    "\n",
    "**ë°©ë²• 1: ëª¨ë¸ëª… ìë™ ì‚¬ìš© (ê¸°ë³¸ê°’, ê¶Œì¥) â­**\n",
    "```python\n",
    "# ì„¹ì…˜ 4ì˜ CFG í´ë˜ìŠ¤ì—ì„œ\n",
    "CFG.model_name = 'efficientnet_b0'\n",
    "CFG.experiment_prefix = None  # Noneìœ¼ë¡œ ìœ ì§€ (ê¸°ë³¸ê°’)\n",
    "CFG.experiment_name = None    # Noneìœ¼ë¡œ ìœ ì§€ (ê¸°ë³¸ê°’)\n",
    "```\n",
    "â†’ ê²°ê³¼: `efficientnet_b0_001`, `efficientnet_b0_002` ...\n",
    "\n",
    "**ë°©ë²• 2: ì»¤ìŠ¤í…€ prefix ì‚¬ìš©**\n",
    "```python\n",
    "# ëª¨ë¸ëª… ëŒ€ì‹  ë‹¤ë¥¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ\n",
    "CFG.experiment_prefix = 'baseline'\n",
    "```\n",
    "â†’ ê²°ê³¼: `baseline_001`, `baseline_002` ...\n",
    "\n",
    "**ë°©ë²• 3: ì™„ì „íˆ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í—˜ëª… ì§€ì •**\n",
    "```python\n",
    "# íŠ¹ì • ì‹¤í—˜ì— ì˜ë¯¸ìˆëŠ” ì´ë¦„ì„ ë¶™ì´ê³  ì‹¶ì„ ë•Œ\n",
    "CFG.experiment_name = 'final_submission_v1'\n",
    "```\n",
    "â†’ ê²°ê³¼: `final_submission_v1` (ì§€ì •í•œ ì´ë¦„ ê·¸ëŒ€ë¡œ)\n",
    "\n",
    "#### ğŸ’¡ ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ í™œìš© ì˜ˆì‹œ\n",
    "\n",
    "**ì‹œë‚˜ë¦¬ì˜¤ 1: ë‹¤ì–‘í•œ ëª¨ë¸ ë¹„êµ (ìë™ ê´€ë¦¬)**\n",
    "```python\n",
    "# EfficientNet B0 ì‹¤í—˜\n",
    "CFG.model_name = 'efficientnet_b0'  # â† ì´ê²ƒë§Œ ë°”ê¾¸ë©´ ë¨!\n",
    "# â†’ efficientnet_b0_001, efficientnet_b0_002 ...\n",
    "\n",
    "# EfficientNet B3 ì‹¤í—˜\n",
    "CFG.model_name = 'efficientnet_b3'  # â† ì´ê²ƒë§Œ ë°”ê¾¸ë©´ ë¨!\n",
    "# â†’ efficientnet_b3_001, efficientnet_b3_002 ...\n",
    "\n",
    "# ResNet50 ì‹¤í—˜\n",
    "CFG.model_name = 'resnet50'  # â† ì´ê²ƒë§Œ ë°”ê¾¸ë©´ ë¨!\n",
    "# â†’ resnet50_001, resnet50_002 ...\n",
    "```\n",
    "\n",
    "**ì‹œë‚˜ë¦¬ì˜¤ 2: ì „ëµë³„ ì‹¤í—˜ (ì»¤ìŠ¤í…€ prefix)**\n",
    "```python\n",
    "# ê¸°ë³¸ ë² ì´ìŠ¤ë¼ì¸\n",
    "CFG.experiment_prefix = 'baseline'\n",
    "# â†’ baseline_001, baseline_002 ...\n",
    "\n",
    "# ê°•í•œ ë°ì´í„° ì¦ê°• ì‹¤í—˜\n",
    "CFG.experiment_prefix = 'strong_augment'\n",
    "# â†’ strong_augment_001, strong_augment_002 ...\n",
    "\n",
    "# ì•™ìƒë¸” ì‹¤í—˜\n",
    "CFG.experiment_prefix = 'ensemble'\n",
    "# â†’ ensemble_001, ensemble_002 ...\n",
    "```\n",
    "\n",
    "**ì‹œë‚˜ë¦¬ì˜¤ 3: ê°™ì€ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜**\n",
    "```python\n",
    "# EfficientNet B0ë¡œ ì—¬ëŸ¬ í•™ìŠµë¥  ì‹¤í—˜\n",
    "CFG.model_name = 'efficientnet_b0'\n",
    "CFG.experiment_prefix = 'effb0_lr_tuning'  # ì»¤ìŠ¤í…€ prefix ì‚¬ìš©\n",
    "\n",
    "CFG.learning_rate = 1e-4\n",
    "# â†’ effb0_lr_tuning_001\n",
    "\n",
    "CFG.learning_rate = 1e-5\n",
    "# â†’ effb0_lr_tuning_002\n",
    "\n",
    "CFG.learning_rate = 5e-5\n",
    "# â†’ effb0_lr_tuning_003\n",
    "```\n",
    "\n",
    "#### ğŸ“Š ì‹¤í–‰ ì˜ˆì‹œ\n",
    "\n",
    "**ì²« ë²ˆì§¸ ì‹¤í—˜ (EfficientNet B0):**\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b0'\n",
    "```\n",
    "ì¶œë ¥:\n",
    "```\n",
    "============================================================\n",
    "WandB Run initialized: efficientnet_b0_001\n",
    "WandB URL: https://wandb.ai/username/document-classification/runs/xxx\n",
    "============================================================\n",
    "```\n",
    "\n",
    "**ê°™ì€ ëª¨ë¸ë¡œ ë‘ ë²ˆì§¸ ì‹¤í—˜:**\n",
    "```\n",
    "============================================================\n",
    "WandB Run initialized: efficientnet_b0_002\n",
    "WandB URL: https://wandb.ai/username/document-classification/runs/yyy\n",
    "============================================================\n",
    "```\n",
    "\n",
    "**ëª¨ë¸ ë³€ê²½ í›„ ì‹¤í—˜:**\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b3'\n",
    "```\n",
    "ì¶œë ¥:\n",
    "```\n",
    "============================================================\n",
    "WandB Run initialized: efficientnet_b3_001\n",
    "WandB URL: https://wandb.ai/username/document-classification/runs/zzz\n",
    "============================================================\n",
    "```\n",
    "\n",
    "#### âš™ï¸ ì‘ë™ ì›ë¦¬\n",
    "1. `CFG.experiment_prefix = None`ì´ë©´ â†’ `CFG.model_name`ì„ ìë™ìœ¼ë¡œ ì‚¬ìš©\n",
    "2. `CFG.experiment_name = None`ì´ë©´ â†’ ìë™ ë²ˆí˜¸ ë¶€ì—¬ ëª¨ë“œ í™œì„±í™”\n",
    "3. WandB APIë¥¼ í†µí•´ í”„ë¡œì íŠ¸ì˜ ê¸°ì¡´ ì‹¤í—˜ë“¤ì„ í™•ì¸\n",
    "4. prefixë¡œ ì‹œì‘í•˜ëŠ” ì‹¤í—˜ ì¤‘ ê°€ì¥ í° ë²ˆí˜¸ ì°¾ê¸°\n",
    "5. ê°€ì¥ í° ë²ˆí˜¸ + 1ë¡œ ìƒˆ ì‹¤í—˜ëª… ìƒì„±\n",
    "6. í•´ë‹¹ prefixì˜ ì²« ì‹¤í—˜ì´ë©´ 001ë¶€í„° ì‹œì‘\n",
    "\n",
    "#### ğŸ¯ ì‹¤ì „ íŒ\n",
    "- **ì´ˆê°„ë‹¨ ì‚¬ìš©**: `CFG.model_name`ë§Œ ë°”ê¾¸ë©´ ëª¨ë“  ê²Œ ìë™ì…ë‹ˆë‹¤! â­\n",
    "- **ëª¨ë¸ ë¹„êµ**: ê° ëª¨ë¸ë³„ë¡œ ìë™ìœ¼ë¡œ ê·¸ë£¹í™”ë˜ì–´ ë¹„êµê°€ ì‰½ìŠµë‹ˆë‹¤\n",
    "- **ì „ëµ ë¹„êµ**: ê°™ì€ ì „ëµì˜ ì‹¤í—˜ë“¤ì„ ë¬¶ê³  ì‹¶ìœ¼ë©´ `experiment_prefix`ë¥¼ ì§ì ‘ ì§€ì •í•˜ì„¸ìš”\n",
    "- **WandB í•„í„°ë§**: WandB ëŒ€ì‹œë³´ë“œì—ì„œ prefixë³„ë¡œ í•„í„°ë§í•˜ë©´ ë¹„êµê°€ ì‰½ìŠµë‹ˆë‹¤\n",
    "- **ì¬ì‹¤í–‰**: ì½”ë© ëŸ°íƒ€ì„ì´ ëŠê²¨ë„ ë²ˆí˜¸ëŠ” ê³„ì† ì´ì–´ì§‘ë‹ˆë‹¤ (WandB ì„œë²„ì— ì €ì¥ë˜ë¯€ë¡œ)\n",
    "\n",
    "#### ğŸ“‹ ë¹ ë¥¸ ì°¸ì¡° í…Œì´ë¸”\n",
    "\n",
    "| ìƒí™© | ì„¤ì • ë°©ë²• | ê²°ê³¼ ì˜ˆì‹œ |\n",
    "|------|-----------|-----------|\n",
    "| **ê¸°ë³¸ ì‚¬ìš© (ëª¨ë¸ëª… ìë™)** | `CFG.model_name = 'efficientnet_b0'`ë§Œ ì„¤ì • | `efficientnet_b0_001` |\n",
    "| **ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½** | `CFG.model_name = 'resnet50'`ìœ¼ë¡œ ë³€ê²½ | `resnet50_001` |\n",
    "| ì „ëµë³„ ê·¸ë£¹í™” | `CFG.experiment_prefix = 'baseline'` | `baseline_001` |\n",
    "| ìˆ˜ë™ ì´ë¦„ ì§€ì • | `CFG.experiment_name = 'my_best_model'` | `my_best_model` |\n",
    "| í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | `CFG.experiment_prefix = 'lr_tuning'` | `lr_tuning_001` |\n",
    "\n",
    "#### ğŸš€ ì‹¤ì „ ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ\n",
    "\n",
    "**1ì¼ì°¨: EfficientNet B0 ì‹¤í—˜**\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b0'\n",
    "# í•™ìŠµ ì‹¤í–‰ â†’ efficientnet_b0_001\n",
    "```\n",
    "\n",
    "**2ì¼ì°¨: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b0'  # ê·¸ëŒ€ë¡œ\n",
    "CFG.learning_rate = 5e-5  # ë³€ê²½\n",
    "# í•™ìŠµ ì‹¤í–‰ â†’ efficientnet_b0_002\n",
    "```\n",
    "\n",
    "**3ì¼ì°¨: ë” í° ëª¨ë¸ë¡œ ì‹¤í—˜**\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b3'  # ëª¨ë¸ë§Œ ë³€ê²½!\n",
    "# í•™ìŠµ ì‹¤í–‰ â†’ efficientnet_b3_001\n",
    "```\n",
    "\n",
    "**4ì¼ì°¨: ResNetë„ ì‹œë„**\n",
    "```python\n",
    "CFG.model_name = 'resnet50'  # ëª¨ë¸ë§Œ ë³€ê²½!\n",
    "# í•™ìŠµ ì‹¤í–‰ â†’ resnet50_001\n",
    "```\n",
    "\n",
    "**5ì¼ì°¨: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¬í•™ìŠµ**\n",
    "```python\n",
    "CFG.model_name = 'efficientnet_b3'\n",
    "CFG.experiment_name = 'final_submission'  # ìˆ˜ë™ ì§€ì •\n",
    "# í•™ìŠµ ì‹¤í–‰ â†’ final_submission\n",
    "```\n",
    "\n",
    "### 6. WandB ì¼ë°˜ ì‚¬ìš©ë²•\n",
    "1. **ì²« ì‹¤í–‰ì‹œ**: `wandb.login()` ì‹¤í–‰ í›„ https://wandb.ai/authorize ì—ì„œ API í‚¤ ë³µì‚¬/ë¶™ì—¬ë„£ê¸°\n",
    "2. **ì‹¤í—˜ ì¶”ì **: í•™ìŠµ ì¤‘ WandB ëŒ€ì‹œë³´ë“œì—ì„œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\n",
    "3. **ê²°ê³¼ í™•ì¸**: í•™ìŠµ ì™„ë£Œ í›„ WandB URLì—ì„œ ëª¨ë“  ë©”íŠ¸ë¦­ê³¼ ì°¨íŠ¸ í™•ì¸\n",
    "4. **ì‹¤í—˜ ë¹„êµ**: ì—¬ëŸ¬ ì‹¤í—˜ì„ ì„ íƒí•´ì„œ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}