{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 문서 이미지 분류 - Albumentations 버전\n\n## 대회 정보\n- **Task**: 문서 이미지 분류 (건강보험증, 여권 등)\n- **Train Data**: ~1,500장 | **Test Data**: ~3,000장\n- **Metric**: Macro F1 Score | **Framework**: PyTorch + Albumentations\n\n## Albumentations 장점\n- 🚀 **더 빠른 속도** (transforms 대비 10-100배)\n- 🎨 **80개 이상 증강 기법**\n- 📄 **문서 특화** (GridDistortion, Perspective 등)\n\n---\n\n## 🎯 추천 모델 (실험 순서)\n\n### 1단계: Baseline ⭐⭐⭐⭐⭐\n```python\nCFG.model_name = 'efficientnet_b0'\nCFG.augmentation_level = 'medium'\n```\n**파라미터**: 5M | **속도**: ~1분/epoch | **용도**: 3가지 증강 레벨 테스트\n\n### 2단계: 성능 향상 ⭐⭐⭐⭐⭐\n```python\nCFG.model_name = 'efficientnet_b1'  # 추천 1순위\n# 또는\nCFG.model_name = 'efficientnet_b2'  # 추천 2순위\n```\n**B1**: 7M, ~1.5분/epoch, B0 대비 +2~3% 향상  \n**B2**: 9M, ~2분/epoch, B0 대비 +3~5% 향상\n\n### 3단계: 최신 아키텍처 ⭐⭐⭐⭐⭐\n```python\nCFG.model_name = 'convnext_tiny'\n```\n**파라미터**: 28M | **속도**: ~2.5분/epoch | **특징**: 최신(2022), B0 대비 +5~7% 향상\n\n### 4단계: 성능 극대화 ⭐⭐⭐⭐\n```python\nCFG.model_name = 'efficientnet_b3'  # 1순위\nCFG.augmentation_level = 'heavy'    # 큰 모델엔 강한 증강\n# 또는\nCFG.model_name = 'convnext_small'   # 2순위 (최고 성능)\n```\n**B3**: 12M, ~2.5분/epoch, B0 대비 +5~8% 향상  \n**ConvNeXt-Small**: 50M, ~4분/epoch, B0 대비 +7~10% 향상\n\n### 5단계: Transformer (선택) ⭐⭐\n```python\nCFG.model_name = 'vit_base_patch16_224'\nCFG.augmentation_level = 'heavy'  # 필수!\n# 또는\nCFG.model_name = 'swin_base_patch4_window7_224'\nCFG.augmentation_level = 'heavy'  # 필수!\n```\n**ViT**: 86M, ~5분/epoch | **Swin**: 88M, ~5분/epoch  \n**주의**: 1,500장에선 과적합 위험 높음, Heavy 증강 필수, **비추천**\n\n---\n\n## ⚠️ 비추천 모델\n- `resnet50` / `resnet101` - EfficientNet보다 비효율적\n- `mobilenetv3_large_100` - 속도 빠르지만 성능 낮음\n- `vit` / `swin` - 데이터 부족 시 과적합 (10,000장 이상일 때 추천)\n\n---\n\n## 🚀 실험 시나리오\n\n### 시나리오 1: 빠른 실험 (2시간)\n1. B0 + Light → 30분 (F1: 0.75)\n2. B0 + Medium → 30분 (F1: 0.78)\n3. B0 + Heavy → 30분 (F1: 0.81)\n4. B1 + Heavy → 40분 (F1: 0.84)\n\n### 시나리오 2: 균형 실험 (4시간)\nB0(3가지 증강) → B1 → B2 → ConvNeXt-Tiny → 최고 모델 재학습\n\n### 시나리오 3: 최고 성능 (하루)\nB0 증강 최적화 → B1/B2 → ConvNeXt-Tiny → B3+Heavy → ConvNeXt-Small → 앙상블\n\n---\n\n## 💡 증강 레벨 가이드\n\n| 레벨 | 언제 사용? | 특징 |\n|------|-----------|------|\n| **Light** | 데이터 깨끗/충분 | 빠름, 원본 유지 |\n| **Medium** ⭐ | 대부분 경우 (권장) | 균형, 현실적 변형 |\n| **Heavy** | 데이터 부족/과적합 | 최대 일반화 |\n\n**팁**: 작은 모델(B0/B1) → medium, 큰 모델(B3/ConvNeXt/Transformer) → heavy\n\n---\n\n## 🔧 트러블슈팅\n\n### GPU 메모리 부족 (OOM Error)\n**해결 방법 (우선순위 순):**\n1. `CFG.batch_size = 16` (추천, 이미지 사이즈 유지) ⭐\n2. `CFG.batch_size = 8` (더 안전)\n3. Mixed Precision 사용: `torch.cuda.amp.autocast()`\n4. **최후의 수단**: `CFG.img_size = 256` (권장 안 함, 성능 저하)\n\n**⚠️ 주의**: 문서 이미지는 `img_size`를 256 이하로 줄이지 마세요! 글자가 안 보여 분류가 불가능합니다.\n\n### 학습이 너무 느림\n- Early Stopping이 있으므로 자동으로 최적화됨 (~15 epoch)\n- 수동 조정: `CFG.epochs = 20`\n\n### 성능이 plateau (정체)\n- 모델 크기 키우기보다 **증강/하이퍼파라미터 튜닝** 먼저!\n- Learning rate 조정, 증강 레벨 변경 시도"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install timm wandb albumentations -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Albumentations 임포트\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import wandb\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'Albumentations version: {A.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 시드 고정 (재현성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 구글 드라이브 마운트 (선택사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 드라이브 마운트 (데이터나 모델을 드라이브에 저장하려면 실행)\n",
    "# 실행하면 인증 링크가 나타나고, 권한 승인 후 코드를 붙여넣으면 됩니다.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"구글 드라이브가 /content/drive 에 마운트되었습니다.\")\n",
    "print(\"데이터 경로 예시: /content/drive/MyDrive/your_data_folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WandB 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB 로그인 (처음 실행시 API 키 입력 필요)\n",
    "# https://wandb.ai/authorize 에서 API 키 발급\n",
    "wandb.login()\n",
    "\n",
    "# 프로젝트명은 실제 대회명으로 변경하세요\n",
    "WANDB_PROJECT = \"document-classification\"\n",
    "WANDB_ENTITY = None  # 팀 계정 사용시 팀명 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델별 권장 이미지 사이즈 (문서 이미지 최적화)\n# 문서 이미지는 텍스트와 세밀한 디테일이 중요하므로 일반 이미지보다 큰 사이즈 사용\nMODEL_IMG_SIZES = {\n    'efficientnet_b0': 384,   # 기본 224 → 384로 증가\n    'efficientnet_b1': 416,   # 기본 240 → 416으로 증가\n    'efficientnet_b2': 448,   # 기본 260 → 448로 증가\n    'efficientnet_b3': 512,   # 기본 300 → 512로 증가\n    'efficientnet_b4': 512,   # 기본 380 → 512 유지\n    'convnext_tiny': 384,     # 기본 224 → 384로 증가\n    'convnext_small': 384,    # 기본 224 → 384로 증가\n    'vit_base_patch16_224': 384,  # 기본 224 → 384로 증가\n    'swin_base_patch4_window7_224': 384,  # 기본 224 → 384로 증가\n}\n\nclass CFG:\n    # 데이터 경로\n    train_dir = './data/train'  # 학습 이미지 폴더\n    test_dir = './data/test'    # 테스트 이미지 폴더\n    \n    # 모델 설정\n    model_name = 'efficientnet_b0'  # timm 모델명\n    num_classes = 10  # 실제 클래스 개수로 변경 필요\n    img_size = MODEL_IMG_SIZES.get(model_name, 384)  # 모델별 권장 사이즈 자동 적용 (문서 이미지용)\n    \n    # 학습 설정\n    epochs = 30\n    batch_size = 32\n    learning_rate = 1e-4\n    weight_decay = 1e-5\n    \n    # Early Stopping 설정\n    early_stopping_patience = 3  # 3 epoch 동안 개선 없으면 중단\n    early_stopping_min_delta = 0.0001  # F1 차이 0.01% 미만은 개선 아님\n    \n    # 데이터 분할\n    val_ratio = 0.2\n    \n    # Albumentations 증강 강도 설정\n    # 'light': 약한 증강 (문서가 깨끗한 경우)\n    # 'medium': 중간 증강 (기본값, 권장)\n    # 'heavy': 강한 증강 (데이터가 매우 부족하거나 다양성이 필요한 경우)\n    augmentation_level = 'medium'\n    \n    # 모델 저장 경로\n    save_to_drive = True  # 구글 드라이브에 저장 여부\n    drive_model_dir = '/content/drive/MyDrive/document_classification/models'  # 드라이브 저장 경로\n    local_model_path = 'best_model.pth'  # 로컬 저장 경로\n    \n    # WandB 설정\n    use_wandb = True\n    wandb_project = WANDB_PROJECT\n    wandb_entity = WANDB_ENTITY\n    experiment_name = None  # None이면 자동으로 번호 부여\n    \n    # 실험명 접두사 설정\n    # None이면 모델명 사용, 직접 지정하면 커스텀 prefix 사용\n    experiment_prefix = None  # 예: 'albumentation', 'heavy_aug' 등\n    \n    # 기타\n    num_workers = 2\n    seed = 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Albumentations 데이터셋 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Albumentations를 사용하는 데이터셋 클래스\n",
    "    \n",
    "    주의: Albumentations는 numpy 배열을 입력으로 받으므로\n",
    "    PIL Image를 numpy 배열로 변환해야 합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Albumentations는 numpy 배열 또는 OpenCV 이미지를 입력으로 받음\n",
    "        # OpenCV로 읽으면 BGR이므로 RGB로 변환 필요\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            # Albumentations는 dict 형태로 반환\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Albumentations 데이터 증강 설정\n",
    "\n",
    "### 문서 이미지에 특화된 증강 기법\n",
    "\n",
    "#### Light (약한 증강)\n",
    "- 깨끗한 문서 이미지에 적합\n",
    "- 기본적인 색상 조정과 약간의 회전만 적용\n",
    "\n",
    "#### Medium (중간 증강) - **권장**\n",
    "- 대부분의 문서 이미지 분류에 적합\n",
    "- 현실적인 변형들을 시뮬레이션\n",
    "- 조명 변화, 그림자, 약간의 왜곡 등\n",
    "\n",
    "#### Heavy (강한 증강)\n",
    "- 데이터가 매우 부족하거나 높은 다양성이 필요한 경우\n",
    "- 강한 왜곡, 노이즈, 컷아웃 등 포함\n",
    "- 과도한 증강은 오히려 성능 저하를 일으킬 수 있으므로 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(img_size=224, level='medium'):\n",
    "    \"\"\"\n",
    "    문서 이미지 분류에 특화된 Albumentations 학습용 증강\n",
    "    \n",
    "    Args:\n",
    "        img_size: 입력 이미지 크기\n",
    "        level: 증강 강도 ('light', 'medium', 'heavy')\n",
    "    \"\"\"\n",
    "    \n",
    "    if level == 'light':\n",
    "        return A.Compose([\n",
    "            # 기본 리사이즈\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # 약한 회전 (문서가 약간 기울어진 경우)\n",
    "            A.Rotate(limit=5, p=0.5),\n",
    "            \n",
    "            # 기본 색상 조정\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.1,\n",
    "                contrast_limit=0.1,\n",
    "                p=0.5\n",
    "            ),\n",
    "            \n",
    "            # 정규화 및 텐서 변환\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    elif level == 'medium':\n",
    "        return A.Compose([\n",
    "            # 기본 리사이즈\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # 수평 뒤집기 (일부 문서는 대칭인 경우)\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            \n",
    "            # 회전 (문서가 기울어진 경우)\n",
    "            A.Rotate(limit=10, border_mode=cv2.BORDER_CONSTANT, value=255, p=0.5),\n",
    "            \n",
    "            # 원근 변환 (문서를 비스듬히 촬영한 경우)\n",
    "            A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "            \n",
    "            # 색상 및 밝기 조정 (조명 변화)\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.2,\n",
    "                    contrast_limit=0.2,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=10,\n",
    "                    sat_shift_limit=20,\n",
    "                    val_shift_limit=10,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.CLAHE(clip_limit=2.0, p=1.0),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            # 그림자 효과 (조명이 불균일한 경우)\n",
    "            A.RandomShadow(p=0.2),\n",
    "            \n",
    "            # 약간의 블러 (초점이 맞지 않은 경우)\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "                A.MotionBlur(blur_limit=3, p=1.0),\n",
    "            ], p=0.2),\n",
    "            \n",
    "            # 약한 노이즈\n",
    "            A.GaussNoise(var_limit=(10.0, 30.0), p=0.2),\n",
    "            \n",
    "            # 정규화 및 텐서 변환\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    elif level == 'heavy':\n",
    "        return A.Compose([\n",
    "            # 기본 리사이즈\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # 수평/수직 뒤집기\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            \n",
    "            # 강한 회전\n",
    "            A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=255, p=0.7),\n",
    "            \n",
    "            # 강한 원근/왜곡 변환\n",
    "            A.OneOf([\n",
    "                A.Perspective(scale=(0.05, 0.15), p=1.0),\n",
    "                A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),\n",
    "                A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            # 강한 색상 변환\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.3,\n",
    "                    contrast_limit=0.3,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=20,\n",
    "                    sat_shift_limit=30,\n",
    "                    val_shift_limit=20,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.CLAHE(clip_limit=4.0, p=1.0),\n",
    "                A.ColorJitter(p=1.0),\n",
    "            ], p=0.7),\n",
    "            \n",
    "            # 그림자 및 조명 효과\n",
    "            A.RandomShadow(p=0.3),\n",
    "            \n",
    "            # 블러 효과\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                A.MotionBlur(blur_limit=5, p=1.0),\n",
    "                A.MedianBlur(blur_limit=5, p=1.0),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # 노이즈\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(20.0, 50.0), p=1.0),\n",
    "                A.ISONoise(p=1.0),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # 컷아웃 (일부 영역 제거)\n",
    "            A.CoarseDropout(\n",
    "                max_holes=8,\n",
    "                max_height=int(img_size * 0.1),\n",
    "                max_width=int(img_size * 0.1),\n",
    "                fill_value=255,\n",
    "                p=0.3\n",
    "            ),\n",
    "            \n",
    "            # 정규화 및 텐서 변환\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown augmentation level: {level}. Use 'light', 'medium', or 'heavy'.\")\n",
    "\n",
    "\n",
    "def get_valid_transforms(img_size=224):\n",
    "    \"\"\"\n",
    "    검증/테스트용 변환 (증강 없음)\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "# Transform 생성\n",
    "train_transform = get_train_transforms(\n",
    "    img_size=CFG.img_size,\n",
    "    level=CFG.augmentation_level\n",
    ")\n",
    "val_transform = get_valid_transforms(img_size=CFG.img_size)\n",
    "\n",
    "print(f\"Augmentation level: {CFG.augmentation_level}\")\n",
    "print(f\"Train transforms: {len(train_transform)} operations\")\n",
    "print(f\"Valid transforms: {len(val_transform)} operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📌 Albumentations 증강 레벨 선택 가이드\n",
    "\n",
    "### 🎯 어떤 레벨을 선택해야 할까요?\n",
    "\n",
    "**CFG 클래스의 `augmentation_level` 파라미터를 변경하면 됩니다!**\n",
    "\n",
    "```python\n",
    "# 섹션 5의 CFG 클래스에서\n",
    "CFG.augmentation_level = 'medium'  # 이 한 줄만 변경하면 됩니다!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 레벨별 특징 및 추천 상황\n",
    "\n",
    "#### 🟢 **Light** - 약한 증강\n",
    "```python\n",
    "CFG.augmentation_level = 'light'\n",
    "```\n",
    "\n",
    "**적용 증강:**\n",
    "- 약한 회전 (±5도)\n",
    "- 기본 밝기/대비 조정\n",
    "\n",
    "**추천 상황:**\n",
    "- ✅ 문서 이미지가 **매우 깨끗하고 품질이 좋은 경우**\n",
    "- ✅ 데이터가 **충분히 많은 경우** (1000장 이상)\n",
    "- ✅ 학습 데이터가 **실제 테스트 환경과 매우 유사한 경우**\n",
    "- ✅ 과도한 증강으로 인해 성능이 저하되는 경우\n",
    "\n",
    "**장점:** 빠른 학습 속도, 원본 데이터의 특성 유지  \n",
    "**단점:** 일반화 성능이 낮을 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "#### 🟡 **Medium** - 중간 증강 ⭐ **권장!**\n",
    "```python\n",
    "CFG.augmentation_level = 'medium'  # 기본값\n",
    "```\n",
    "\n",
    "**적용 증강:**\n",
    "- 수평 뒤집기 (30%)\n",
    "- 회전 (±10도)\n",
    "- 원근 변환 (Perspective)\n",
    "- 밝기/대비/색조 조정\n",
    "- 그림자 효과\n",
    "- 약한 블러 및 노이즈\n",
    "\n",
    "**추천 상황:**\n",
    "- ✅ **대부분의 문서 이미지 분류 작업** (가장 범용적)\n",
    "- ✅ 데이터가 **중간 정도인 경우** (500-2000장)\n",
    "- ✅ 실제 환경에서 **조명이나 각도가 다양한 경우**\n",
    "- ✅ 스캔/촬영 품질이 **일정하지 않은 경우**\n",
    "- ✅ **첫 실험으로 권장!** 이후 성능을 보고 조정\n",
    "\n",
    "**장점:** 성능과 일반화의 균형, 현실적인 변형 시뮬레이션  \n",
    "**단점:** 없음 (가장 안정적)\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔴 **Heavy** - 강한 증강\n",
    "```python\n",
    "CFG.augmentation_level = 'heavy'\n",
    "```\n",
    "\n",
    "**적용 증강:**\n",
    "- 수평/수직 뒤집기\n",
    "- 강한 회전 (±15도)\n",
    "- 강한 원근/그리드 왜곡 변환\n",
    "- 강한 색상 변환\n",
    "- 그림자 및 강한 블러\n",
    "- 노이즈 추가\n",
    "- **CoarseDropout** (일부 영역 제거)\n",
    "\n",
    "**추천 상황:**\n",
    "- ✅ 데이터가 **매우 부족한 경우** (500장 미만)\n",
    "- ✅ **과적합(Overfitting)이 심한 경우**\n",
    "- ✅ 테스트 환경이 **학습 환경과 매우 다른 경우**\n",
    "- ✅ 문서 품질이 **다양하고 예측 불가능한 경우**\n",
    "- ⚠️ Medium으로 시도 후 성능이 낮을 때 고려\n",
    "\n",
    "**장점:** 최대 일반화, 강한 정규화 효과  \n",
    "**단점:** 과도한 증강으로 성능 저하 가능, 학습 시간 증가\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 실험 워크플로우 추천\n",
    "\n",
    "#### **단계 1: Medium으로 시작 (Baseline)**\n",
    "```python\n",
    "CFG.augmentation_level = 'medium'\n",
    "```\n",
    "→ 학습 후 성능 확인\n",
    "\n",
    "#### **단계 2: 성능에 따라 조정**\n",
    "\n",
    "**Case A: Train 정확도 >> Val 정확도 (과적합)**\n",
    "```python\n",
    "CFG.augmentation_level = 'heavy'  # 더 강한 증강으로 정규화\n",
    "```\n",
    "\n",
    "**Case B: Train/Val 모두 높지만 Test가 낮음**\n",
    "```python\n",
    "CFG.augmentation_level = 'heavy'  # 더 다양한 변형 학습\n",
    "```\n",
    "\n",
    "**Case C: Train/Val 모두 낮음**\n",
    "```python\n",
    "CFG.augmentation_level = 'light'  # 증강 약화, 모델/하이퍼파라미터 점검\n",
    "```\n",
    "\n",
    "**Case D: 성능이 충분히 좋음**\n",
    "```python\n",
    "# Medium 유지 또는 Light로 미세 조정\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 WandB에서 비교하기\n",
    "\n",
    "**각 레벨별로 실험을 진행하면 자동으로 다른 이름으로 저장됩니다:**\n",
    "\n",
    "```python\n",
    "# Light 실험\n",
    "CFG.augmentation_level = 'light'\n",
    "# → efficientnet_b0_alb_light_001\n",
    "\n",
    "# Medium 실험\n",
    "CFG.augmentation_level = 'medium'\n",
    "# → efficientnet_b0_alb_medium_001\n",
    "\n",
    "# Heavy 실험\n",
    "CFG.augmentation_level = 'heavy'\n",
    "# → efficientnet_b0_alb_heavy_001\n",
    "```\n",
    "\n",
    "**WandB 대시보드에서 세 실험을 선택하고 비교하세요!**\n",
    "\n",
    "---\n",
    "\n",
    "### 🎨 증강 시각화로 확인하기\n",
    "\n",
    "**섹션 9**의 증강 시각화 코드를 실행하면 각 레벨의 증강 결과를 눈으로 확인할 수 있습니다!\n",
    "\n",
    "```python\n",
    "# 데이터 로드 후 실행\n",
    "visualize_augmentations(train_dataset, idx=0, samples=4)\n",
    "```\n",
    "\n",
    "**증강이 너무 강해 보이면** → Light로 변경  \n",
    "**증강이 너무 약해 보이면** → Heavy로 변경\n",
    "\n",
    "---\n",
    "\n",
    "### 💻 빠른 변경 예시\n",
    "\n",
    "섹션 5의 CFG 클래스로 돌아가서 한 줄만 수정:\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    # ... (다른 설정들)\n",
    "    \n",
    "    # 이 한 줄만 바꾸면 됩니다!\n",
    "    augmentation_level = 'medium'  # 'light', 'medium', 'heavy' 중 선택\n",
    "    \n",
    "    # ... (다른 설정들)\n",
    "```\n",
    "\n",
    "그리고 노트북을 처음부터 다시 실행! 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. 데이터 로드 및 전처리\n\n**주의**: 이 코드는 제공된 데이터셋 구조에 맞게 작성되었습니다.\n\n데이터 구조:\n```\ndata/\n├── train.csv       (ID, target)\n├── meta.csv        (target, class_name)\n├── train/          (이미지 파일들)\n│   ├── image1.jpg\n│   ├── image2.jpg\n│   └── ...\n└── test/           (테스트 이미지 파일들)\n    ├── test1.jpg\n    └── ...\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 로드 함수 (train.csv + meta.csv 기반)\ndef load_data_from_csv(train_csv_path, meta_csv_path, train_dir):\n    \"\"\"\n    train.csv와 meta.csv를 읽어서 데이터 로드\n\n    Args:\n        train_csv_path: 'train.csv' 경로 (ID, target)\n        meta_csv_path: 'meta.csv' 경로 (target, class_name)\n        train_dir: 학습 이미지 폴더 경로\n\n    Returns:\n        image_paths, labels, class_to_idx\n    \"\"\"\n    # train.csv 읽기 (ID, target)\n    train_df = pd.read_csv(train_csv_path)\n    print(f\"train.csv loaded: {len(train_df)} entries\")\n\n    # meta.csv 읽기 (target, class_name)\n    meta_df = pd.read_csv(meta_csv_path)\n    print(f\"meta.csv loaded: {len(meta_df)} classes\")\n\n    # class_to_idx 매핑 생성 (class_name → target)\n    class_to_idx = dict(zip(meta_df['class_name'], meta_df['target']))\n\n    # idx_to_class 매핑 생성 (target → class_name)\n    idx_to_class = dict(zip(meta_df['target'], meta_df['class_name']))\n\n    # 이미지 경로와 라벨 리스트 생성\n    image_paths = []\n    labels = []\n    missing_count = 0\n\n    for _, row in train_df.iterrows():\n        img_id = row['ID']\n        target = row['target']\n\n        # 이미지 경로 생성 (확장자가 포함되어 있을 수도 있음)\n        img_path = os.path.join(train_dir, img_id)\n\n        # 파일이 실제로 존재하는지 확인\n        if os.path.exists(img_path):\n            image_paths.append(img_path)\n            labels.append(target)\n        else:\n            # 확장자를 시도해보기\n            found = False\n            for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n                img_path_with_ext = os.path.join(train_dir, img_id + ext) if not img_id.endswith(ext) else img_path\n                if os.path.exists(img_path_with_ext):\n                    image_paths.append(img_path_with_ext)\n                    labels.append(target)\n                    found = True\n                    break\n\n            if not found:\n                missing_count += 1\n\n    if missing_count > 0:\n        print(f\"Warning: {missing_count} images from train.csv not found in {train_dir}\")\n\n    print(f\"\\nSuccessfully loaded {len(image_paths)} images\")\n    print(f\"Number of classes: {len(class_to_idx)}\")\n    print(f\"\\nClasses:\")\n    for class_name, idx in sorted(class_to_idx.items(), key=lambda x: x[1]):\n        print(f\"  [{idx}] {class_name}\")\n\n    return image_paths, labels, class_to_idx\n\n# 학습 데이터 로드\ntrain_csv_path = './data/train.csv'\nmeta_csv_path = './data/meta.csv'\n\nif os.path.exists(train_csv_path) and os.path.exists(meta_csv_path) and os.path.exists(CFG.train_dir):\n    train_paths, train_labels, class_to_idx = load_data_from_csv(train_csv_path, meta_csv_path, CFG.train_dir)\n\n    # CFG.num_classes 업데이트\n    CFG.num_classes = len(class_to_idx)\nelse:\n    missing_files = []\n    if not os.path.exists(train_csv_path):\n        missing_files.append(train_csv_path)\n    if not os.path.exists(meta_csv_path):\n        missing_files.append(meta_csv_path)\n    if not os.path.exists(CFG.train_dir):\n        missing_files.append(CFG.train_dir)\n\n    print(f\"Error: Missing required files/directories:\")\n    for f in missing_files:\n        print(f\"  - {f}\")\n    print(\"\\nPlease upload your data or modify the paths in CFG.\")"
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\ndef visualize_random_samples(image_paths, labels, class_to_idx, num_samples=5):\n    \"\"\"\n    데이터셋에서 랜덤으로 샘플을 추출하여 시각화\n    \n    Args:\n        image_paths: 이미지 경로 리스트\n        labels: 라벨 리스트\n        class_to_idx: 클래스명-인덱스 매핑 딕셔너리\n        num_samples: 시각화할 샘플 개수 (기본값: 5)\n    \"\"\"\n    # 인덱스-클래스명 매핑\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    \n    # 랜덤 인덱스 선택\n    random_indices = random.sample(range(len(image_paths)), min(num_samples, len(image_paths)))\n    \n    # 플롯 생성\n    fig, axes = plt.subplots(1, len(random_indices), figsize=(4 * len(random_indices), 4))\n    if len(random_indices) == 1:\n        axes = [axes]\n    \n    for idx, img_idx in enumerate(random_indices):\n        # 이미지 로드\n        img_path = image_paths[img_idx]\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 클래스명 가져오기\n        label_idx = labels[img_idx]\n        class_name = idx_to_class[label_idx]\n        \n        # 시각화\n        axes[idx].imshow(image)\n        axes[idx].set_title(f'Class: {class_name}\\n({os.path.basename(img_path)})', fontsize=10)\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# 데이터가 로드된 경우 랜덤 샘플 시각화\nif 'train_paths' in locals() and len(train_paths) > 0:\n    print(f\"\\n{'='*60}\")\n    print(\"데이터셋에서 랜덤으로 5개 샘플 추출 (원본 이미지)\")\n    print(f\"{'='*60}\\n\")\n    visualize_random_samples(train_paths, train_labels, class_to_idx, num_samples=5)\n    \n    # 클래스별 분포 출력\n    print(f\"\\n{'='*60}\")\n    print(\"클래스별 이미지 개수:\")\n    print(f\"{'='*60}\")\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    class_counts = {}\n    for label in train_labels:\n        class_name = idx_to_class[label]\n        class_counts[class_name] = class_counts.get(class_name, 0) + 1\n    \n    for class_name in sorted(class_counts.keys()):\n        print(f\"  {class_name}: {class_counts[class_name]:4d} images\")\n    print(f\"{'='*60}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8-1. 데이터셋 확인 (랜덤 샘플 5개 시각화)\n\n로드한 데이터가 올바른지 확인하기 위해 각 클래스에서 랜덤으로 샘플을 추출하여 확인합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 8-2. 추가 데이터 분석 (EDA)\n\n데이터의 특성을 더 자세히 파악하기 위한 시각화입니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\n\ndef visualize_class_distribution(labels, class_to_idx):\n    \"\"\"Visualize the distribution of classes in the dataset\"\"\"\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    class_names = [idx_to_class[label] for label in labels]\n    class_counts = Counter(class_names)\n    \n    classes = list(class_counts.keys())\n    counts = list(class_counts.values())\n    \n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(classes, counts)\n    plt.xlabel('Class')\n    plt.ylabel('Number of Images')\n    plt.title('Class Distribution in Training Dataset')\n    plt.xticks(rotation=45, ha='right')\n    \n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{int(height)}',\n                ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nClass distribution:\")\n    for class_name, count in sorted(class_counts.items()):\n        print(f\"{class_name}: {count} images\")\n\ndef analyze_image_resolutions(image_paths, num_samples=None):\n    \"\"\"Analyze and visualize image resolution distribution\"\"\"\n    from PIL import Image\n    \n    if num_samples:\n        sample_paths = np.random.choice(image_paths, min(num_samples, len(image_paths)), replace=False)\n    else:\n        sample_paths = image_paths\n    \n    widths = []\n    heights = []\n    \n    for img_path in sample_paths:\n        try:\n            with Image.open(img_path) as img:\n                w, h = img.size\n                widths.append(w)\n                heights.append(h)\n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    axes[0].hist(widths, bins=30, edgecolor='black')\n    axes[0].set_xlabel('Width (pixels)')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Image Width Distribution')\n    axes[0].axvline(np.mean(widths), color='r', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n    axes[0].legend()\n    \n    axes[1].hist(heights, bins=30, edgecolor='black')\n    axes[1].set_xlabel('Height (pixels)')\n    axes[1].set_ylabel('Frequency')\n    axes[1].set_title('Image Height Distribution')\n    axes[1].axvline(np.mean(heights), color='r', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nResolution Statistics (from {len(widths)} images):\")\n    print(f\"Width  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.0f}\")\n    print(f\"Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.0f}\")\n\ndef visualize_class_grid(image_paths, labels, class_to_idx, samples_per_class=3):\n    \"\"\"Visualize random samples from each class in a grid\"\"\"\n    from PIL import Image\n    \n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    \n    class_images = {class_name: [] for class_name in class_to_idx.keys()}\n    for img_path, label in zip(image_paths, labels):\n        class_name = idx_to_class[label]\n        class_images[class_name].append(img_path)\n    \n    num_classes = len(class_to_idx)\n    fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(samples_per_class * 3, num_classes * 3))\n    \n    if num_classes == 1:\n        axes = axes.reshape(1, -1)\n    \n    for idx, (class_name, img_paths) in enumerate(sorted(class_images.items())):\n        sample_paths = np.random.choice(img_paths, min(samples_per_class, len(img_paths)), replace=False)\n        \n        for col, img_path in enumerate(sample_paths):\n            try:\n                img = Image.open(img_path)\n                axes[idx, col].imshow(img)\n                axes[idx, col].axis('off')\n                if col == 0:\n                    axes[idx, col].set_ylabel(class_name, rotation=0, ha='right', va='center', fontsize=12)\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n        \n        for col in range(len(sample_paths), samples_per_class):\n            axes[idx, col].axis('off')\n    \n    plt.suptitle('Random Samples from Each Class', fontsize=16, y=0.995)\n    plt.tight_layout()\n    plt.show()\n\nif 'train_paths' in locals() and len(train_paths) > 0:\n    print(\"=\" * 50)\n    print(\"EXPLORATORY DATA ANALYSIS\")\n    print(\"=\" * 50)\n    \n    print(\"\\n1. Class Distribution Analysis\")\n    print(\"-\" * 50)\n    visualize_class_distribution(train_labels, class_to_idx)\n    \n    print(\"\\n2. Image Resolution Analysis\")\n    print(\"-\" * 50)\n    analyze_image_resolutions(train_paths, num_samples=500)\n    \n    print(\"\\n3. Visual Sample Grid\")\n    print(\"-\" * 50)\n    visualize_class_grid(train_paths, train_labels, class_to_idx, samples_per_class=3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation 분할\n",
    "if 'train_paths' in locals():\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_paths, train_labels, \n",
    "        test_size=CFG.val_ratio, \n",
    "        random_state=CFG.seed,\n",
    "        stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_paths)}\")\n",
    "    print(f\"Validation size: {len(val_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 데이터로더 생성\n",
    "if 'train_paths' in locals():\n",
    "    train_dataset = AlbumentationsDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = AlbumentationsDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=CFG.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 증강 결과 시각화 (선택사항)\n",
    "\n",
    "증강이 어떻게 적용되는지 확인해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, samples=5):\n",
    "    \"\"\"\n",
    "    데이터셋의 증강 결과를 시각화\n",
    "    \n",
    "    Args:\n",
    "        dataset: AlbumentationsDataset 객체\n",
    "        idx: 시각화할 이미지의 인덱스\n",
    "        samples: 생성할 증강 샘플 수\n",
    "    \"\"\"\n",
    "    # 원본 이미지 로드\n",
    "    img_path = dataset.image_paths[idx]\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 플롯 생성\n",
    "    fig, axes = plt.subplots(1, samples + 1, figsize=(4 * (samples + 1), 4))\n",
    "    \n",
    "    # 원본 이미지\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # 증강된 이미지들\n",
    "    for i in range(samples):\n",
    "        augmented = dataset.transform(image=original_image)\n",
    "        aug_image = augmented['image']\n",
    "        \n",
    "        # 텐서를 numpy로 변환하고 정규화 해제\n",
    "        if isinstance(aug_image, torch.Tensor):\n",
    "            aug_image = aug_image.permute(1, 2, 0).numpy()\n",
    "            # 정규화 해제\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            aug_image = std * aug_image + mean\n",
    "            aug_image = np.clip(aug_image, 0, 1)\n",
    "        \n",
    "        axes[i + 1].imshow(aug_image)\n",
    "        axes[i + 1].set_title(f'Augmented {i+1}', fontsize=12)\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 증강 시각화 (데이터가 로드된 경우)\n",
    "if 'train_dataset' in locals() and len(train_dataset) > 0:\n",
    "    print(f\"Visualizing augmentations with level: {CFG.augmentation_level}\")\n",
    "    visualize_augmentations(train_dataset, idx=0, samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. WandB Run 초기화 (학습 시작 전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_experiment_number(project_name, prefix, entity=None):\n",
    "    \"\"\"WandB에서 기존 실험들을 확인하고 다음 번호를 반환\"\"\"\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        # 프로젝트의 모든 run 가져오기\n",
    "        if entity:\n",
    "            runs = api.runs(f\"{entity}/{project_name}\")\n",
    "        else:\n",
    "            runs = api.runs(project_name)\n",
    "        \n",
    "        # prefix로 시작하는 run들의 번호 추출\n",
    "        numbers = []\n",
    "        for run in runs:\n",
    "            if run.name.startswith(prefix):\n",
    "                try:\n",
    "                    # 'prefix_123' 형태에서 123 추출\n",
    "                    num = int(run.name.split('_')[-1])\n",
    "                    numbers.append(num)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # 가장 큰 번호 + 1 반환\n",
    "        next_num = max(numbers) + 1 if numbers else 1\n",
    "        return next_num\n",
    "    except:\n",
    "        # API 접근 실패시 001부터 시작\n",
    "        return 1\n",
    "\n",
    "# WandB Run 초기화\n",
    "if CFG.use_wandb:\n",
    "    # experiment_prefix가 None이면 모델명 사용 (자동)\n",
    "    if CFG.experiment_prefix is None:\n",
    "        actual_prefix = f\"{CFG.model_name}_alb_{CFG.augmentation_level}\"\n",
    "    else:\n",
    "        actual_prefix = CFG.experiment_prefix\n",
    "    \n",
    "    # 실험명 자동 생성\n",
    "    if CFG.experiment_name is None:\n",
    "        exp_num = get_next_experiment_number(\n",
    "            CFG.wandb_project, \n",
    "            actual_prefix,\n",
    "            CFG.wandb_entity\n",
    "        )\n",
    "        CFG.experiment_name = f\"{actual_prefix}_{exp_num:03d}\"\n",
    "    \n",
    "    run = wandb.init(\n",
    "        project=CFG.wandb_project,\n",
    "        entity=CFG.wandb_entity,\n",
    "        name=CFG.experiment_name,\n",
    "        config={\n",
    "            \"model_name\": CFG.model_name,\n",
    "            \"num_classes\": CFG.num_classes,\n",
    "            \"img_size\": CFG.img_size,\n",
    "            \"epochs\": CFG.epochs,\n",
    "            \"batch_size\": CFG.batch_size,\n",
    "            \"learning_rate\": CFG.learning_rate,\n",
    "            \"weight_decay\": CFG.weight_decay,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"scheduler\": \"CosineAnnealingLR\",\n",
    "            \"val_ratio\": CFG.val_ratio,\n",
    "            \"seed\": CFG.seed,\n",
    "            \"augmentation\": \"albumentations\",\n",
    "            \"augmentation_level\": CFG.augmentation_level,\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"WandB Run initialized: {run.name}\")\n",
    "    print(f\"WandB URL: {run.url}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"WandB is disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=True):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        \n",
    "        # 모델의 classifier 부분 수정\n",
    "        if 'efficientnet' in model_name:\n",
    "            in_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(in_features, num_classes)\n",
    "        elif 'resnet' in model_name:\n",
    "            in_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(in_features, num_classes)\n",
    "        elif 'vit' in model_name:\n",
    "            in_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 생성\n",
    "model = DocumentClassifier(\n",
    "    model_name=CFG.model_name, \n",
    "    num_classes=CFG.num_classes, \n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: {CFG.model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# WandB에 모델 아키텍처 로깅\n",
    "if CFG.use_wandb:\n",
    "    wandb.watch(model, log='all', log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 손실 함수 및 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 학습 및 검증 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # 배치별 메트릭 계산\n",
    "        batch_loss = running_loss / (batch_idx + 1)\n",
    "        batch_acc = 100. * correct / total\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': batch_loss,\n",
    "            'acc': batch_acc\n",
    "        })\n",
    "        \n",
    "        # WandB 로깅 (매 배치마다)\n",
    "        if CFG.use_wandb:\n",
    "            wandb.log({\n",
    "                'train/batch_loss': loss.item(),\n",
    "                'train/batch_acc': 100. * predicted.eq(labels).sum().item() / labels.size(0),\n",
    "                'train/step': epoch * len(train_loader) + batch_idx\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    # Macro F1 Score 계산\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # 클래스별 F1 Score 계산\n",
    "    per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
    "    \n",
    "    # Confusion Matrix 계산\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, macro_f1, per_class_f1, cm, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "best_f1 = 0.0\npatience_counter = 0  # Early Stopping 카운터\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_f1': []\n}\n\n# 구글 드라이브 저장 경로 생성\nif CFG.save_to_drive:\n    os.makedirs(CFG.drive_model_dir, exist_ok=True)\n    print(f\"모델 저장 경로: {CFG.drive_model_dir}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Early Stopping: Patience={CFG.early_stopping_patience}, Min Delta={CFG.early_stopping_min_delta}\")\nprint(f\"{'='*60}\\n\")\n\nfor epoch in range(CFG.epochs):\n    print(f\"\\nEpoch {epoch+1}/{CFG.epochs}\")\n    print(\"-\" * 50)\n    \n    # 학습\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n    \n    # 검증\n    val_loss, val_f1, per_class_f1, cm, val_preds, val_labels = validate(model, val_loader, criterion, device)\n    \n    # 스케줄러 업데이트\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # 결과 저장\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Macro F1: {val_f1:.4f}\")\n    print(f\"Learning Rate: {current_lr:.6f}\")\n    \n    # WandB 로깅 (에폭별)\n    if CFG.use_wandb:\n        # 기본 메트릭\n        log_dict = {\n            'epoch': epoch + 1,\n            'train/epoch_loss': train_loss,\n            'train/epoch_acc': train_acc,\n            'val/loss': val_loss,\n            'val/macro_f1': val_f1,\n            'learning_rate': current_lr,\n            'early_stopping/patience_counter': patience_counter,\n        }\n        \n        # 클래스별 F1 Score\n        if 'class_to_idx' in locals():\n            idx_to_class = {v: k for k, v in class_to_idx.items()}\n            for idx, f1 in enumerate(per_class_f1):\n                class_name = idx_to_class.get(idx, f'class_{idx}')\n                log_dict[f'val/f1_{class_name}'] = f1\n        \n        # Confusion Matrix (5 에폭마다)\n        if (epoch + 1) % 5 == 0:\n            log_dict['val/confusion_matrix'] = wandb.plot.confusion_matrix(\n                probs=None,\n                y_true=val_labels,\n                preds=val_preds,\n                class_names=[idx_to_class.get(i, f'class_{i}') for i in range(CFG.num_classes)] if 'idx_to_class' in locals() else None\n            )\n        \n        wandb.log(log_dict)\n    \n    # Early Stopping 체크 및 베스트 모델 저장\n    if val_f1 > best_f1 + CFG.early_stopping_min_delta:\n        # 성능 개선됨\n        best_f1 = val_f1\n        patience_counter = 0  # 카운터 리셋\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_f1': best_f1,\n        }\n        \n        # 로컬에 저장\n        torch.save(checkpoint, CFG.local_model_path)\n        print(f\"✓ Best model saved locally! (F1: {best_f1:.4f})\")\n        \n        # 구글 드라이브에 저장\n        if CFG.save_to_drive:\n            # 실험명을 파일명에 포함\n            if CFG.use_wandb and CFG.experiment_name:\n                drive_model_path = f\"{CFG.drive_model_dir}/{CFG.experiment_name}_f1_{best_f1:.4f}.pth\"\n            else:\n                drive_model_path = f\"{CFG.drive_model_dir}/best_model_alb_{CFG.augmentation_level}_f1_{best_f1:.4f}.pth\"\n            \n            torch.save(checkpoint, drive_model_path)\n            print(f\"✓ Best model saved to drive: {drive_model_path}\")\n        \n        # WandB에 베스트 모델 저장\n        if CFG.use_wandb:\n            artifact = wandb.Artifact(\n                name=f'model-{run.id}',\n                type='model',\n                description=f'Best model with F1: {best_f1:.4f}',\n                metadata={\n                    'epoch': epoch + 1,\n                    'val_f1': val_f1,\n                    'val_loss': val_loss,\n                    'augmentation_level': CFG.augmentation_level,\n                }\n            )\n            artifact.add_file(CFG.local_model_path)\n            wandb.log_artifact(artifact)\n    else:\n        # 성능 개선 없음\n        patience_counter += 1\n        print(f\"⚠ No improvement. Patience: {patience_counter}/{CFG.early_stopping_patience}\")\n        \n        # Early Stopping 체크\n        if patience_counter >= CFG.early_stopping_patience:\n            print(f\"\\n{'='*60}\")\n            print(f\"Early Stopping triggered at epoch {epoch+1}\")\n            print(f\"Best Validation Macro F1: {best_f1:.4f}\")\n            print(f\"{'='*60}\")\n            break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training completed!\")\nprint(f\"Best Validation Macro F1: {best_f1:.4f}\")\nprint(f\"Augmentation level: {CFG.augmentation_level}\")\nprint(f\"Total epochs: {epoch+1}\")\nif patience_counter >= CFG.early_stopping_patience:\n    print(f\"Stopped early due to no improvement for {CFG.early_stopping_patience} epochs\")\nprint(f\"{'='*50}\")\n\n# 최종 모델 경로 출력\nprint(f\"\\n모델 저장 위치:\")\nprint(f\"  - 로컬: {CFG.local_model_path}\")\nif CFG.save_to_drive:\n    print(f\"  - 드라이브: {CFG.drive_model_dir}/\")\n\n# WandB Run 종료\nif CFG.use_wandb:\n    wandb.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss 그래프\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title(f'Training and Validation Loss (Albumentations {CFG.augmentation_level})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 Score 그래프\n",
    "axes[1].plot(history['val_f1'], label='Val Macro F1', color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Macro F1 Score')\n",
    "axes[1].set_title(f'Validation Macro F1 Score (Albumentations {CFG.augmentation_level})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. 테스트 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Best model loaded (F1: {checkpoint['best_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "def load_test_data(test_dir):\n",
    "    test_paths = []\n",
    "    for img_name in sorted(os.listdir(test_dir)):\n",
    "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            test_paths.append(os.path.join(test_dir, img_name))\n",
    "    return test_paths\n",
    "\n",
    "if os.path.exists(CFG.test_dir):\n",
    "    test_paths = load_test_data(CFG.test_dir)\n",
    "    print(f\"Total test images: {len(test_paths)}\")\n",
    "else:\n",
    "    print(f\"Warning: {CFG.test_dir} does not exist!\")\n",
    "    test_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 및 로더 생성\n",
    "if test_paths:\n",
    "    test_dataset = AlbumentationsDataset(test_paths, labels=None, transform=val_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 함수\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader, desc='Predicting'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 추론 실행\n",
    "if test_paths:\n",
    "    predictions = predict(model, test_loader, device)\n",
    "    print(f\"Prediction completed: {len(predictions)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성 (형식은 대회 규정에 맞게 수정)\n",
    "if test_paths and predictions:\n",
    "    # 클래스 인덱스를 클래스 이름으로 변환\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'image': [os.path.basename(path) for path in test_paths],\n",
    "        'label': [idx_to_class[pred] for pred in predictions]\n",
    "    })\n",
    "    \n",
    "    submission_filename = f'submission_alb_{CFG.augmentation_level}.csv'\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"\\nSubmission file saved: {submission_filename}\")\n",
    "    print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. 고급 팁 및 추가 개선 아이디어\n",
    "\n",
    "### 🔬 커스텀 증강 만들기\n",
    "\n",
    "기본 제공되는 3가지 레벨 외에 직접 커스텀 증강을 만들 수 있습니다:\n",
    "\n",
    "```python\n",
    "# 예시: 문서 스캔 특화 증강\n",
    "custom_transform = A.Compose([\n",
    "    A.Resize(CFG.img_size, CFG.img_size),\n",
    "    \n",
    "    # 스캔 시 발생하는 노이즈\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "        A.ISONoise(p=1.0),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    # 구겨진 문서 효과\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n",
    "    \n",
    "    # 팩스/복사기 블러\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Test Time Augmentation (TTA)\n",
    "\n",
    "추론 시에도 증강을 적용하여 **앙상블 효과**를 얻을 수 있습니다:\n",
    "\n",
    "```python\n",
    "def predict_with_tta(model, image, transforms, n_augmentations=5):\n",
    "    \"\"\"\n",
    "    TTA를 적용한 예측 함수\n",
    "    같은 이미지를 여러 번 증강하고 예측을 평균내어 더 안정적인 결과 획득\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_augmentations):\n",
    "            augmented = transforms(image=image)\n",
    "            img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "            output = model(img_tensor)\n",
    "            predictions.append(output.softmax(dim=1))\n",
    "    \n",
    "    # 평균 예측\n",
    "    avg_prediction = torch.stack(predictions).mean(dim=0)\n",
    "    return avg_prediction.argmax(dim=1).item()\n",
    "```\n",
    "\n",
    "**TTA 사용 시 주의사항:**\n",
    "- 추론 시간이 n_augmentations배 증가\n",
    "- 일반적으로 0.5-2% 성능 향상\n",
    "- 최종 제출 시에만 사용 권장\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 성능 향상을 위한 추가 아이디어\n",
    "\n",
    "#### 1. **증강 파라미터 튜닝**\n",
    "```python\n",
    "# WandB Sweep으로 최적 증강 강도 찾기\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'parameters': {\n",
    "        'augmentation_level': {\n",
    "            'values': ['light', 'medium', 'heavy']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. **MixUp / CutMix**\n",
    "```python\n",
    "# Albumentations의 MixUp transform\n",
    "A.MixUp(alpha=0.2, p=0.5)\n",
    "```\n",
    "\n",
    "#### 3. **클래스별 맞춤 증강**\n",
    "```python\n",
    "# 불균형 데이터셋의 경우 소수 클래스에 더 강한 증강 적용\n",
    "if label in minority_classes:\n",
    "    transform = heavy_transform\n",
    "else:\n",
    "    transform = medium_transform\n",
    "```\n",
    "\n",
    "#### 4. **AutoAugment / RandAugment**\n",
    "```python\n",
    "# 자동으로 최적 증강 정책 학습\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# Albumentations도 AutoAugment 지원\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 디버깅 팁\n",
    "\n",
    "**증강이 너무 강해서 성능이 떨어진다면:**\n",
    "1. 섹션 9의 시각화로 증강 결과 확인\n",
    "2. `p` (확률) 파라미터 조정\n",
    "3. 더 약한 레벨로 변경\n",
    "\n",
    "**학습이 불안정하다면:**\n",
    "1. Learning rate 감소\n",
    "2. Batch size 증가\n",
    "3. 증강 강도 감소\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 참고 자료\n",
    "\n",
    "- [Albumentations 공식 문서](https://albumentations.ai/docs/)\n",
    "- [Albumentations 예제 모음](https://albumentations.ai/docs/examples/)\n",
    "- [문서 이미지 증강 Best Practices](https://arxiv.org/abs/2106.08322)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}