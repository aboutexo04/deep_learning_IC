{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ - Albumentations ë²„ì „\n\n## ëŒ€íšŒ ì •ë³´\n- **Task**: ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ (ê±´ê°•ë³´í—˜ì¦, ì—¬ê¶Œ ë“±)\n- **Train Data**: ~1,500ì¥ | **Test Data**: ~3,000ì¥\n- **Metric**: Macro F1 Score | **Framework**: PyTorch + Albumentations\n\n## Albumentations ì¥ì \n- ğŸš€ **ë” ë¹ ë¥¸ ì†ë„** (transforms ëŒ€ë¹„ 10-100ë°°)\n- ğŸ¨ **80ê°œ ì´ìƒ ì¦ê°• ê¸°ë²•**\n- ğŸ“„ **ë¬¸ì„œ íŠ¹í™”** (GridDistortion, Perspective ë“±)\n\n---\n\n## ğŸ¯ ì¶”ì²œ ëª¨ë¸ (ì‹¤í—˜ ìˆœì„œ)\n\n### 1ë‹¨ê³„: Baseline â­â­â­â­â­\n```python\nCFG.model_name = 'efficientnet_b0'\nCFG.augmentation_level = 'medium'\n```\n**íŒŒë¼ë¯¸í„°**: 5M | **ì†ë„**: ~1ë¶„/epoch | **ìš©ë„**: 3ê°€ì§€ ì¦ê°• ë ˆë²¨ í…ŒìŠ¤íŠ¸\n\n### 2ë‹¨ê³„: ì„±ëŠ¥ í–¥ìƒ â­â­â­â­â­\n```python\nCFG.model_name = 'efficientnet_b1'  # ì¶”ì²œ 1ìˆœìœ„\n# ë˜ëŠ”\nCFG.model_name = 'efficientnet_b2'  # ì¶”ì²œ 2ìˆœìœ„\n```\n**B1**: 7M, ~1.5ë¶„/epoch, B0 ëŒ€ë¹„ +2~3% í–¥ìƒ  \n**B2**: 9M, ~2ë¶„/epoch, B0 ëŒ€ë¹„ +3~5% í–¥ìƒ\n\n### 3ë‹¨ê³„: ìµœì‹  ì•„í‚¤í…ì²˜ â­â­â­â­â­\n```python\nCFG.model_name = 'convnext_tiny'\n```\n**íŒŒë¼ë¯¸í„°**: 28M | **ì†ë„**: ~2.5ë¶„/epoch | **íŠ¹ì§•**: ìµœì‹ (2022), B0 ëŒ€ë¹„ +5~7% í–¥ìƒ\n\n### 4ë‹¨ê³„: ì„±ëŠ¥ ê·¹ëŒ€í™” â­â­â­â­\n```python\nCFG.model_name = 'efficientnet_b3'  # 1ìˆœìœ„\nCFG.augmentation_level = 'heavy'    # í° ëª¨ë¸ì—” ê°•í•œ ì¦ê°•\n# ë˜ëŠ”\nCFG.model_name = 'convnext_small'   # 2ìˆœìœ„ (ìµœê³  ì„±ëŠ¥)\n```\n**B3**: 12M, ~2.5ë¶„/epoch, B0 ëŒ€ë¹„ +5~8% í–¥ìƒ  \n**ConvNeXt-Small**: 50M, ~4ë¶„/epoch, B0 ëŒ€ë¹„ +7~10% í–¥ìƒ\n\n### 5ë‹¨ê³„: Transformer (ì„ íƒ) â­â­\n```python\nCFG.model_name = 'vit_base_patch16_224'\nCFG.augmentation_level = 'heavy'  # í•„ìˆ˜!\n# ë˜ëŠ”\nCFG.model_name = 'swin_base_patch4_window7_224'\nCFG.augmentation_level = 'heavy'  # í•„ìˆ˜!\n```\n**ViT**: 86M, ~5ë¶„/epoch | **Swin**: 88M, ~5ë¶„/epoch  \n**ì£¼ì˜**: 1,500ì¥ì—ì„  ê³¼ì í•© ìœ„í—˜ ë†’ìŒ, Heavy ì¦ê°• í•„ìˆ˜, **ë¹„ì¶”ì²œ**\n\n---\n\n## âš ï¸ ë¹„ì¶”ì²œ ëª¨ë¸\n- `resnet50` / `resnet101` - EfficientNetë³´ë‹¤ ë¹„íš¨ìœ¨ì \n- `mobilenetv3_large_100` - ì†ë„ ë¹ ë¥´ì§€ë§Œ ì„±ëŠ¥ ë‚®ìŒ\n- `vit` / `swin` - ë°ì´í„° ë¶€ì¡± ì‹œ ê³¼ì í•© (10,000ì¥ ì´ìƒì¼ ë•Œ ì¶”ì²œ)\n\n---\n\n## ğŸš€ ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤\n\n### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ ì‹¤í—˜ (2ì‹œê°„)\n1. B0 + Light â†’ 30ë¶„ (F1: 0.75)\n2. B0 + Medium â†’ 30ë¶„ (F1: 0.78)\n3. B0 + Heavy â†’ 30ë¶„ (F1: 0.81)\n4. B1 + Heavy â†’ 40ë¶„ (F1: 0.84)\n\n### ì‹œë‚˜ë¦¬ì˜¤ 2: ê· í˜• ì‹¤í—˜ (4ì‹œê°„)\nB0(3ê°€ì§€ ì¦ê°•) â†’ B1 â†’ B2 â†’ ConvNeXt-Tiny â†’ ìµœê³  ëª¨ë¸ ì¬í•™ìŠµ\n\n### ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœê³  ì„±ëŠ¥ (í•˜ë£¨)\nB0 ì¦ê°• ìµœì í™” â†’ B1/B2 â†’ ConvNeXt-Tiny â†’ B3+Heavy â†’ ConvNeXt-Small â†’ ì•™ìƒë¸”\n\n---\n\n## ğŸ’¡ ì¦ê°• ë ˆë²¨ ê°€ì´ë“œ\n\n| ë ˆë²¨ | ì–¸ì œ ì‚¬ìš©? | íŠ¹ì§• |\n|------|-----------|------|\n| **Light** | ë°ì´í„° ê¹¨ë—/ì¶©ë¶„ | ë¹ ë¦„, ì›ë³¸ ìœ ì§€ |\n| **Medium** â­ | ëŒ€ë¶€ë¶„ ê²½ìš° (ê¶Œì¥) | ê· í˜•, í˜„ì‹¤ì  ë³€í˜• |\n| **Heavy** | ë°ì´í„° ë¶€ì¡±/ê³¼ì í•© | ìµœëŒ€ ì¼ë°˜í™” |\n\n**íŒ**: ì‘ì€ ëª¨ë¸(B0/B1) â†’ medium, í° ëª¨ë¸(B3/ConvNeXt/Transformer) â†’ heavy\n\n---\n\n## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n\n### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM Error)\n**í•´ê²° ë°©ë²• (ìš°ì„ ìˆœìœ„ ìˆœ):**\n1. `CFG.batch_size = 16` (ì¶”ì²œ, ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ìœ ì§€) â­\n2. `CFG.batch_size = 8` (ë” ì•ˆì „)\n3. Mixed Precision ì‚¬ìš©: `torch.cuda.amp.autocast()`\n4. **ìµœí›„ì˜ ìˆ˜ë‹¨**: `CFG.img_size = 256` (ê¶Œì¥ ì•ˆ í•¨, ì„±ëŠ¥ ì €í•˜)\n\n**âš ï¸ ì£¼ì˜**: ë¬¸ì„œ ì´ë¯¸ì§€ëŠ” `img_size`ë¥¼ 256 ì´í•˜ë¡œ ì¤„ì´ì§€ ë§ˆì„¸ìš”! ê¸€ìê°€ ì•ˆ ë³´ì—¬ ë¶„ë¥˜ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼\n- Early Stoppingì´ ìˆìœ¼ë¯€ë¡œ ìë™ìœ¼ë¡œ ìµœì í™”ë¨ (~15 epoch)\n- ìˆ˜ë™ ì¡°ì •: `CFG.epochs = 20`\n\n### ì„±ëŠ¥ì´ plateau (ì •ì²´)\n- ëª¨ë¸ í¬ê¸° í‚¤ìš°ê¸°ë³´ë‹¤ **ì¦ê°•/í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** ë¨¼ì €!\n- Learning rate ì¡°ì •, ì¦ê°• ë ˆë²¨ ë³€ê²½ ì‹œë„"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install timm wandb albumentations -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Albumentations ì„í¬íŠ¸\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import wandb\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'Albumentations version: {A.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ (ì„ íƒì‚¬í•­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ (ë°ì´í„°ë‚˜ ëª¨ë¸ì„ ë“œë¼ì´ë¸Œì— ì €ì¥í•˜ë ¤ë©´ ì‹¤í–‰)\n",
    "# ì‹¤í–‰í•˜ë©´ ì¸ì¦ ë§í¬ê°€ ë‚˜íƒ€ë‚˜ê³ , ê¶Œí•œ ìŠ¹ì¸ í›„ ì½”ë“œë¥¼ ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"êµ¬ê¸€ ë“œë¼ì´ë¸Œê°€ /content/drive ì— ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"ë°ì´í„° ê²½ë¡œ ì˜ˆì‹œ: /content/drive/MyDrive/your_data_folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WandB ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB ë¡œê·¸ì¸ (ì²˜ìŒ ì‹¤í–‰ì‹œ API í‚¤ ì…ë ¥ í•„ìš”)\n",
    "# https://wandb.ai/authorize ì—ì„œ API í‚¤ ë°œê¸‰\n",
    "wandb.login()\n",
    "\n",
    "# í”„ë¡œì íŠ¸ëª…ì€ ì‹¤ì œ ëŒ€íšŒëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”\n",
    "WANDB_PROJECT = \"document-classification\"\n",
    "WANDB_ENTITY = None  # íŒ€ ê³„ì • ì‚¬ìš©ì‹œ íŒ€ëª… ì…ë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ëª¨ë¸ë³„ ê¶Œì¥ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ (ë¬¸ì„œ ì´ë¯¸ì§€ ìµœì í™”)\n# ë¬¸ì„œ ì´ë¯¸ì§€ëŠ” í…ìŠ¤íŠ¸ì™€ ì„¸ë°€í•œ ë””í…Œì¼ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ì¼ë°˜ ì´ë¯¸ì§€ë³´ë‹¤ í° ì‚¬ì´ì¦ˆ ì‚¬ìš©\nMODEL_IMG_SIZES = {\n    'efficientnet_b0': 384,   # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'efficientnet_b1': 416,   # ê¸°ë³¸ 240 â†’ 416ìœ¼ë¡œ ì¦ê°€\n    'efficientnet_b2': 448,   # ê¸°ë³¸ 260 â†’ 448ë¡œ ì¦ê°€\n    'efficientnet_b3': 512,   # ê¸°ë³¸ 300 â†’ 512ë¡œ ì¦ê°€\n    'efficientnet_b4': 512,   # ê¸°ë³¸ 380 â†’ 512 ìœ ì§€\n    'convnext_tiny': 384,     # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'convnext_small': 384,    # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'vit_base_patch16_224': 384,  # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n    'swin_base_patch4_window7_224': 384,  # ê¸°ë³¸ 224 â†’ 384ë¡œ ì¦ê°€\n}\n\nclass CFG:\n    # ë°ì´í„° ê²½ë¡œ\n    train_dir = './data/train'  # í•™ìŠµ ì´ë¯¸ì§€ í´ë”\n    test_dir = './data/test'    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë”\n    \n    # ëª¨ë¸ ì„¤ì •\n    model_name = 'efficientnet_b0'  # timm ëª¨ë¸ëª…\n    num_classes = 10  # ì‹¤ì œ í´ë˜ìŠ¤ ê°œìˆ˜ë¡œ ë³€ê²½ í•„ìš”\n    img_size = MODEL_IMG_SIZES.get(model_name, 384)  # ëª¨ë¸ë³„ ê¶Œì¥ ì‚¬ì´ì¦ˆ ìë™ ì ìš© (ë¬¸ì„œ ì´ë¯¸ì§€ìš©)\n    \n    # í•™ìŠµ ì„¤ì •\n    epochs = 30\n    batch_size = 32\n    learning_rate = 1e-4\n    weight_decay = 1e-5\n    \n    # Early Stopping ì„¤ì •\n    early_stopping_patience = 3  # 3 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨\n    early_stopping_min_delta = 0.0001  # F1 ì°¨ì´ 0.01% ë¯¸ë§Œì€ ê°œì„  ì•„ë‹˜\n    \n    # ë°ì´í„° ë¶„í• \n    val_ratio = 0.2\n    \n    # Albumentations ì¦ê°• ê°•ë„ ì„¤ì •\n    # 'light': ì•½í•œ ì¦ê°• (ë¬¸ì„œê°€ ê¹¨ë—í•œ ê²½ìš°)\n    # 'medium': ì¤‘ê°„ ì¦ê°• (ê¸°ë³¸ê°’, ê¶Œì¥)\n    # 'heavy': ê°•í•œ ì¦ê°• (ë°ì´í„°ê°€ ë§¤ìš° ë¶€ì¡±í•˜ê±°ë‚˜ ë‹¤ì–‘ì„±ì´ í•„ìš”í•œ ê²½ìš°)\n    augmentation_level = 'medium'\n    \n    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n    save_to_drive = True  # êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥ ì—¬ë¶€\n    drive_model_dir = '/content/drive/MyDrive/document_classification/models'  # ë“œë¼ì´ë¸Œ ì €ì¥ ê²½ë¡œ\n    local_model_path = 'best_model.pth'  # ë¡œì»¬ ì €ì¥ ê²½ë¡œ\n    \n    # WandB ì„¤ì •\n    use_wandb = True\n    wandb_project = WANDB_PROJECT\n    wandb_entity = WANDB_ENTITY\n    experiment_name = None  # Noneì´ë©´ ìë™ìœ¼ë¡œ ë²ˆí˜¸ ë¶€ì—¬\n    \n    # ì‹¤í—˜ëª… ì ‘ë‘ì‚¬ ì„¤ì •\n    # Noneì´ë©´ ëª¨ë¸ëª… ì‚¬ìš©, ì§ì ‘ ì§€ì •í•˜ë©´ ì»¤ìŠ¤í…€ prefix ì‚¬ìš©\n    experiment_prefix = None  # ì˜ˆ: 'albumentation', 'heavy_aug' ë“±\n    \n    # ê¸°íƒ€\n    num_workers = 2\n    seed = 42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Albumentations ë°ì´í„°ì…‹ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Albumentationsë¥¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "    \n",
    "    ì£¼ì˜: AlbumentationsëŠ” numpy ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë¯€ë¡œ\n",
    "    PIL Imageë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # AlbumentationsëŠ” numpy ë°°ì—´ ë˜ëŠ” OpenCV ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ\n",
    "        # OpenCVë¡œ ì½ìœ¼ë©´ BGRì´ë¯€ë¡œ RGBë¡œ ë³€í™˜ í•„ìš”\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            # AlbumentationsëŠ” dict í˜•íƒœë¡œ ë°˜í™˜\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Albumentations ë°ì´í„° ì¦ê°• ì„¤ì •\n",
    "\n",
    "### ë¬¸ì„œ ì´ë¯¸ì§€ì— íŠ¹í™”ëœ ì¦ê°• ê¸°ë²•\n",
    "\n",
    "#### Light (ì•½í•œ ì¦ê°•)\n",
    "- ê¹¨ë—í•œ ë¬¸ì„œ ì´ë¯¸ì§€ì— ì í•©\n",
    "- ê¸°ë³¸ì ì¸ ìƒ‰ìƒ ì¡°ì •ê³¼ ì•½ê°„ì˜ íšŒì „ë§Œ ì ìš©\n",
    "\n",
    "#### Medium (ì¤‘ê°„ ì¦ê°•) - **ê¶Œì¥**\n",
    "- ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ì— ì í•©\n",
    "- í˜„ì‹¤ì ì¸ ë³€í˜•ë“¤ì„ ì‹œë®¬ë ˆì´ì…˜\n",
    "- ì¡°ëª… ë³€í™”, ê·¸ë¦¼ì, ì•½ê°„ì˜ ì™œê³¡ ë“±\n",
    "\n",
    "#### Heavy (ê°•í•œ ì¦ê°•)\n",
    "- ë°ì´í„°ê°€ ë§¤ìš° ë¶€ì¡±í•˜ê±°ë‚˜ ë†’ì€ ë‹¤ì–‘ì„±ì´ í•„ìš”í•œ ê²½ìš°\n",
    "- ê°•í•œ ì™œê³¡, ë…¸ì´ì¦ˆ, ì»·ì•„ì›ƒ ë“± í¬í•¨\n",
    "- ê³¼ë„í•œ ì¦ê°•ì€ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(img_size=224, level='medium'):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ì— íŠ¹í™”ëœ Albumentations í•™ìŠµìš© ì¦ê°•\n",
    "    \n",
    "    Args:\n",
    "        img_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°\n",
    "        level: ì¦ê°• ê°•ë„ ('light', 'medium', 'heavy')\n",
    "    \"\"\"\n",
    "    \n",
    "    if level == 'light':\n",
    "        return A.Compose([\n",
    "            # ê¸°ë³¸ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # ì•½í•œ íšŒì „ (ë¬¸ì„œê°€ ì•½ê°„ ê¸°ìš¸ì–´ì§„ ê²½ìš°)\n",
    "            A.Rotate(limit=5, p=0.5),\n",
    "            \n",
    "            # ê¸°ë³¸ ìƒ‰ìƒ ì¡°ì •\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.1,\n",
    "                contrast_limit=0.1,\n",
    "                p=0.5\n",
    "            ),\n",
    "            \n",
    "            # ì •ê·œí™” ë° í…ì„œ ë³€í™˜\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    elif level == 'medium':\n",
    "        return A.Compose([\n",
    "            # ê¸°ë³¸ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # ìˆ˜í‰ ë’¤ì§‘ê¸° (ì¼ë¶€ ë¬¸ì„œëŠ” ëŒ€ì¹­ì¸ ê²½ìš°)\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            \n",
    "            # íšŒì „ (ë¬¸ì„œê°€ ê¸°ìš¸ì–´ì§„ ê²½ìš°)\n",
    "            A.Rotate(limit=10, border_mode=cv2.BORDER_CONSTANT, value=255, p=0.5),\n",
    "            \n",
    "            # ì›ê·¼ ë³€í™˜ (ë¬¸ì„œë¥¼ ë¹„ìŠ¤ë“¬íˆ ì´¬ì˜í•œ ê²½ìš°)\n",
    "            A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "            \n",
    "            # ìƒ‰ìƒ ë° ë°ê¸° ì¡°ì • (ì¡°ëª… ë³€í™”)\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.2,\n",
    "                    contrast_limit=0.2,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=10,\n",
    "                    sat_shift_limit=20,\n",
    "                    val_shift_limit=10,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.CLAHE(clip_limit=2.0, p=1.0),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            # ê·¸ë¦¼ì íš¨ê³¼ (ì¡°ëª…ì´ ë¶ˆê· ì¼í•œ ê²½ìš°)\n",
    "            A.RandomShadow(p=0.2),\n",
    "            \n",
    "            # ì•½ê°„ì˜ ë¸”ëŸ¬ (ì´ˆì ì´ ë§ì§€ ì•Šì€ ê²½ìš°)\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "                A.MotionBlur(blur_limit=3, p=1.0),\n",
    "            ], p=0.2),\n",
    "            \n",
    "            # ì•½í•œ ë…¸ì´ì¦ˆ\n",
    "            A.GaussNoise(var_limit=(10.0, 30.0), p=0.2),\n",
    "            \n",
    "            # ì •ê·œí™” ë° í…ì„œ ë³€í™˜\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    elif level == 'heavy':\n",
    "        return A.Compose([\n",
    "            # ê¸°ë³¸ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "            A.Resize(img_size, img_size),\n",
    "            \n",
    "            # ìˆ˜í‰/ìˆ˜ì§ ë’¤ì§‘ê¸°\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            \n",
    "            # ê°•í•œ íšŒì „\n",
    "            A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=255, p=0.7),\n",
    "            \n",
    "            # ê°•í•œ ì›ê·¼/ì™œê³¡ ë³€í™˜\n",
    "            A.OneOf([\n",
    "                A.Perspective(scale=(0.05, 0.15), p=1.0),\n",
    "                A.GridDistortion(num_steps=5, distort_limit=0.3, p=1.0),\n",
    "                A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),\n",
    "            ], p=0.5),\n",
    "            \n",
    "            # ê°•í•œ ìƒ‰ìƒ ë³€í™˜\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.3,\n",
    "                    contrast_limit=0.3,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=20,\n",
    "                    sat_shift_limit=30,\n",
    "                    val_shift_limit=20,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.CLAHE(clip_limit=4.0, p=1.0),\n",
    "                A.ColorJitter(p=1.0),\n",
    "            ], p=0.7),\n",
    "            \n",
    "            # ê·¸ë¦¼ì ë° ì¡°ëª… íš¨ê³¼\n",
    "            A.RandomShadow(p=0.3),\n",
    "            \n",
    "            # ë¸”ëŸ¬ íš¨ê³¼\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                A.MotionBlur(blur_limit=5, p=1.0),\n",
    "                A.MedianBlur(blur_limit=5, p=1.0),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # ë…¸ì´ì¦ˆ\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(20.0, 50.0), p=1.0),\n",
    "                A.ISONoise(p=1.0),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # ì»·ì•„ì›ƒ (ì¼ë¶€ ì˜ì—­ ì œê±°)\n",
    "            A.CoarseDropout(\n",
    "                max_holes=8,\n",
    "                max_height=int(img_size * 0.1),\n",
    "                max_width=int(img_size * 0.1),\n",
    "                fill_value=255,\n",
    "                p=0.3\n",
    "            ),\n",
    "            \n",
    "            # ì •ê·œí™” ë° í…ì„œ ë³€í™˜\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown augmentation level: {level}. Use 'light', 'medium', or 'heavy'.\")\n",
    "\n",
    "\n",
    "def get_valid_transforms(img_size=224):\n",
    "    \"\"\"\n",
    "    ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© ë³€í™˜ (ì¦ê°• ì—†ìŒ)\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "\n",
    "# Transform ìƒì„±\n",
    "train_transform = get_train_transforms(\n",
    "    img_size=CFG.img_size,\n",
    "    level=CFG.augmentation_level\n",
    ")\n",
    "val_transform = get_valid_transforms(img_size=CFG.img_size)\n",
    "\n",
    "print(f\"Augmentation level: {CFG.augmentation_level}\")\n",
    "print(f\"Train transforms: {len(train_transform)} operations\")\n",
    "print(f\"Valid transforms: {len(val_transform)} operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Albumentations ì¦ê°• ë ˆë²¨ ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "### ğŸ¯ ì–´ë–¤ ë ˆë²¨ì„ ì„ íƒí•´ì•¼ í• ê¹Œìš”?\n",
    "\n",
    "**CFG í´ë˜ìŠ¤ì˜ `augmentation_level` íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤!**\n",
    "\n",
    "```python\n",
    "# ì„¹ì…˜ 5ì˜ CFG í´ë˜ìŠ¤ì—ì„œ\n",
    "CFG.augmentation_level = 'medium'  # ì´ í•œ ì¤„ë§Œ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ ë ˆë²¨ë³„ íŠ¹ì§• ë° ì¶”ì²œ ìƒí™©\n",
    "\n",
    "#### ğŸŸ¢ **Light** - ì•½í•œ ì¦ê°•\n",
    "```python\n",
    "CFG.augmentation_level = 'light'\n",
    "```\n",
    "\n",
    "**ì ìš© ì¦ê°•:**\n",
    "- ì•½í•œ íšŒì „ (Â±5ë„)\n",
    "- ê¸°ë³¸ ë°ê¸°/ëŒ€ë¹„ ì¡°ì •\n",
    "\n",
    "**ì¶”ì²œ ìƒí™©:**\n",
    "- âœ… ë¬¸ì„œ ì´ë¯¸ì§€ê°€ **ë§¤ìš° ê¹¨ë—í•˜ê³  í’ˆì§ˆì´ ì¢‹ì€ ê²½ìš°**\n",
    "- âœ… ë°ì´í„°ê°€ **ì¶©ë¶„íˆ ë§ì€ ê²½ìš°** (1000ì¥ ì´ìƒ)\n",
    "- âœ… í•™ìŠµ ë°ì´í„°ê°€ **ì‹¤ì œ í…ŒìŠ¤íŠ¸ í™˜ê²½ê³¼ ë§¤ìš° ìœ ì‚¬í•œ ê²½ìš°**\n",
    "- âœ… ê³¼ë„í•œ ì¦ê°•ìœ¼ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê²½ìš°\n",
    "\n",
    "**ì¥ì :** ë¹ ë¥¸ í•™ìŠµ ì†ë„, ì›ë³¸ ë°ì´í„°ì˜ íŠ¹ì„± ìœ ì§€  \n",
    "**ë‹¨ì :** ì¼ë°˜í™” ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŸ¡ **Medium** - ì¤‘ê°„ ì¦ê°• â­ **ê¶Œì¥!**\n",
    "```python\n",
    "CFG.augmentation_level = 'medium'  # ê¸°ë³¸ê°’\n",
    "```\n",
    "\n",
    "**ì ìš© ì¦ê°•:**\n",
    "- ìˆ˜í‰ ë’¤ì§‘ê¸° (30%)\n",
    "- íšŒì „ (Â±10ë„)\n",
    "- ì›ê·¼ ë³€í™˜ (Perspective)\n",
    "- ë°ê¸°/ëŒ€ë¹„/ìƒ‰ì¡° ì¡°ì •\n",
    "- ê·¸ë¦¼ì íš¨ê³¼\n",
    "- ì•½í•œ ë¸”ëŸ¬ ë° ë…¸ì´ì¦ˆ\n",
    "\n",
    "**ì¶”ì²œ ìƒí™©:**\n",
    "- âœ… **ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—…** (ê°€ì¥ ë²”ìš©ì )\n",
    "- âœ… ë°ì´í„°ê°€ **ì¤‘ê°„ ì •ë„ì¸ ê²½ìš°** (500-2000ì¥)\n",
    "- âœ… ì‹¤ì œ í™˜ê²½ì—ì„œ **ì¡°ëª…ì´ë‚˜ ê°ë„ê°€ ë‹¤ì–‘í•œ ê²½ìš°**\n",
    "- âœ… ìŠ¤ìº”/ì´¬ì˜ í’ˆì§ˆì´ **ì¼ì •í•˜ì§€ ì•Šì€ ê²½ìš°**\n",
    "- âœ… **ì²« ì‹¤í—˜ìœ¼ë¡œ ê¶Œì¥!** ì´í›„ ì„±ëŠ¥ì„ ë³´ê³  ì¡°ì •\n",
    "\n",
    "**ì¥ì :** ì„±ëŠ¥ê³¼ ì¼ë°˜í™”ì˜ ê· í˜•, í˜„ì‹¤ì ì¸ ë³€í˜• ì‹œë®¬ë ˆì´ì…˜  \n",
    "**ë‹¨ì :** ì—†ìŒ (ê°€ì¥ ì•ˆì •ì )\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”´ **Heavy** - ê°•í•œ ì¦ê°•\n",
    "```python\n",
    "CFG.augmentation_level = 'heavy'\n",
    "```\n",
    "\n",
    "**ì ìš© ì¦ê°•:**\n",
    "- ìˆ˜í‰/ìˆ˜ì§ ë’¤ì§‘ê¸°\n",
    "- ê°•í•œ íšŒì „ (Â±15ë„)\n",
    "- ê°•í•œ ì›ê·¼/ê·¸ë¦¬ë“œ ì™œê³¡ ë³€í™˜\n",
    "- ê°•í•œ ìƒ‰ìƒ ë³€í™˜\n",
    "- ê·¸ë¦¼ì ë° ê°•í•œ ë¸”ëŸ¬\n",
    "- ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "- **CoarseDropout** (ì¼ë¶€ ì˜ì—­ ì œê±°)\n",
    "\n",
    "**ì¶”ì²œ ìƒí™©:**\n",
    "- âœ… ë°ì´í„°ê°€ **ë§¤ìš° ë¶€ì¡±í•œ ê²½ìš°** (500ì¥ ë¯¸ë§Œ)\n",
    "- âœ… **ê³¼ì í•©(Overfitting)ì´ ì‹¬í•œ ê²½ìš°**\n",
    "- âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ **í•™ìŠµ í™˜ê²½ê³¼ ë§¤ìš° ë‹¤ë¥¸ ê²½ìš°**\n",
    "- âœ… ë¬¸ì„œ í’ˆì§ˆì´ **ë‹¤ì–‘í•˜ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°**\n",
    "- âš ï¸ Mediumìœ¼ë¡œ ì‹œë„ í›„ ì„±ëŠ¥ì´ ë‚®ì„ ë•Œ ê³ ë ¤\n",
    "\n",
    "**ì¥ì :** ìµœëŒ€ ì¼ë°˜í™”, ê°•í•œ ì •ê·œí™” íš¨ê³¼  \n",
    "**ë‹¨ì :** ê³¼ë„í•œ ì¦ê°•ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥, í•™ìŠµ ì‹œê°„ ì¦ê°€\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ ì‹¤í—˜ ì›Œí¬í”Œë¡œìš° ì¶”ì²œ\n",
    "\n",
    "#### **ë‹¨ê³„ 1: Mediumìœ¼ë¡œ ì‹œì‘ (Baseline)**\n",
    "```python\n",
    "CFG.augmentation_level = 'medium'\n",
    "```\n",
    "â†’ í•™ìŠµ í›„ ì„±ëŠ¥ í™•ì¸\n",
    "\n",
    "#### **ë‹¨ê³„ 2: ì„±ëŠ¥ì— ë”°ë¼ ì¡°ì •**\n",
    "\n",
    "**Case A: Train ì •í™•ë„ >> Val ì •í™•ë„ (ê³¼ì í•©)**\n",
    "```python\n",
    "CFG.augmentation_level = 'heavy'  # ë” ê°•í•œ ì¦ê°•ìœ¼ë¡œ ì •ê·œí™”\n",
    "```\n",
    "\n",
    "**Case B: Train/Val ëª¨ë‘ ë†’ì§€ë§Œ Testê°€ ë‚®ìŒ**\n",
    "```python\n",
    "CFG.augmentation_level = 'heavy'  # ë” ë‹¤ì–‘í•œ ë³€í˜• í•™ìŠµ\n",
    "```\n",
    "\n",
    "**Case C: Train/Val ëª¨ë‘ ë‚®ìŒ**\n",
    "```python\n",
    "CFG.augmentation_level = 'light'  # ì¦ê°• ì•½í™”, ëª¨ë¸/í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ê²€\n",
    "```\n",
    "\n",
    "**Case D: ì„±ëŠ¥ì´ ì¶©ë¶„íˆ ì¢‹ìŒ**\n",
    "```python\n",
    "# Medium ìœ ì§€ ë˜ëŠ” Lightë¡œ ë¯¸ì„¸ ì¡°ì •\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š WandBì—ì„œ ë¹„êµí•˜ê¸°\n",
    "\n",
    "**ê° ë ˆë²¨ë³„ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤:**\n",
    "\n",
    "```python\n",
    "# Light ì‹¤í—˜\n",
    "CFG.augmentation_level = 'light'\n",
    "# â†’ efficientnet_b0_alb_light_001\n",
    "\n",
    "# Medium ì‹¤í—˜\n",
    "CFG.augmentation_level = 'medium'\n",
    "# â†’ efficientnet_b0_alb_medium_001\n",
    "\n",
    "# Heavy ì‹¤í—˜\n",
    "CFG.augmentation_level = 'heavy'\n",
    "# â†’ efficientnet_b0_alb_heavy_001\n",
    "```\n",
    "\n",
    "**WandB ëŒ€ì‹œë³´ë“œì—ì„œ ì„¸ ì‹¤í—˜ì„ ì„ íƒí•˜ê³  ë¹„êµí•˜ì„¸ìš”!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¨ ì¦ê°• ì‹œê°í™”ë¡œ í™•ì¸í•˜ê¸°\n",
    "\n",
    "**ì„¹ì…˜ 9**ì˜ ì¦ê°• ì‹œê°í™” ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ê° ë ˆë²¨ì˜ ì¦ê°• ê²°ê³¼ë¥¼ ëˆˆìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "```python\n",
    "# ë°ì´í„° ë¡œë“œ í›„ ì‹¤í–‰\n",
    "visualize_augmentations(train_dataset, idx=0, samples=4)\n",
    "```\n",
    "\n",
    "**ì¦ê°•ì´ ë„ˆë¬´ ê°•í•´ ë³´ì´ë©´** â†’ Lightë¡œ ë³€ê²½  \n",
    "**ì¦ê°•ì´ ë„ˆë¬´ ì•½í•´ ë³´ì´ë©´** â†’ Heavyë¡œ ë³€ê²½\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’» ë¹ ë¥¸ ë³€ê²½ ì˜ˆì‹œ\n",
    "\n",
    "ì„¹ì…˜ 5ì˜ CFG í´ë˜ìŠ¤ë¡œ ëŒì•„ê°€ì„œ í•œ ì¤„ë§Œ ìˆ˜ì •:\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    # ... (ë‹¤ë¥¸ ì„¤ì •ë“¤)\n",
    "    \n",
    "    # ì´ í•œ ì¤„ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤!\n",
    "    augmentation_level = 'medium'  # 'light', 'medium', 'heavy' ì¤‘ ì„ íƒ\n",
    "    \n",
    "    # ... (ë‹¤ë¥¸ ì„¤ì •ë“¤)\n",
    "```\n",
    "\n",
    "ê·¸ë¦¬ê³  ë…¸íŠ¸ë¶ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹¤í–‰! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n\n**ì£¼ì˜**: ì´ ì½”ë“œëŠ” ì œê³µëœ ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë§ê²Œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\në°ì´í„° êµ¬ì¡°:\n```\ndata/\nâ”œâ”€â”€ train.csv       (ID, target)\nâ”œâ”€â”€ meta.csv        (target, class_name)\nâ”œâ”€â”€ train/          (ì´ë¯¸ì§€ íŒŒì¼ë“¤)\nâ”‚   â”œâ”€â”€ image1.jpg\nâ”‚   â”œâ”€â”€ image2.jpg\nâ”‚   â””â”€â”€ ...\nâ””â”€â”€ test/           (í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ë“¤)\n    â”œâ”€â”€ test1.jpg\n    â””â”€â”€ ...\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (train.csv + meta.csv ê¸°ë°˜)\ndef load_data_from_csv(train_csv_path, meta_csv_path, train_dir):\n    \"\"\"\n    train.csvì™€ meta.csvë¥¼ ì½ì–´ì„œ ë°ì´í„° ë¡œë“œ\n\n    Args:\n        train_csv_path: 'train.csv' ê²½ë¡œ (ID, target)\n        meta_csv_path: 'meta.csv' ê²½ë¡œ (target, class_name)\n        train_dir: í•™ìŠµ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ\n\n    Returns:\n        image_paths, labels, class_to_idx\n    \"\"\"\n    # train.csv ì½ê¸° (ID, target)\n    train_df = pd.read_csv(train_csv_path)\n    print(f\"train.csv loaded: {len(train_df)} entries\")\n\n    # meta.csv ì½ê¸° (target, class_name)\n    meta_df = pd.read_csv(meta_csv_path)\n    print(f\"meta.csv loaded: {len(meta_df)} classes\")\n\n    # class_to_idx ë§¤í•‘ ìƒì„± (class_name â†’ target)\n    class_to_idx = dict(zip(meta_df['class_name'], meta_df['target']))\n\n    # idx_to_class ë§¤í•‘ ìƒì„± (target â†’ class_name)\n    idx_to_class = dict(zip(meta_df['target'], meta_df['class_name']))\n\n    # ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n    image_paths = []\n    labels = []\n    missing_count = 0\n\n    for _, row in train_df.iterrows():\n        img_id = row['ID']\n        target = row['target']\n\n        # ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„± (í™•ì¥ìê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ë„ ìˆìŒ)\n        img_path = os.path.join(train_dir, img_id)\n\n        # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n        if os.path.exists(img_path):\n            image_paths.append(img_path)\n            labels.append(target)\n        else:\n            # í™•ì¥ìë¥¼ ì‹œë„í•´ë³´ê¸°\n            found = False\n            for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n                img_path_with_ext = os.path.join(train_dir, img_id + ext) if not img_id.endswith(ext) else img_path\n                if os.path.exists(img_path_with_ext):\n                    image_paths.append(img_path_with_ext)\n                    labels.append(target)\n                    found = True\n                    break\n\n            if not found:\n                missing_count += 1\n\n    if missing_count > 0:\n        print(f\"Warning: {missing_count} images from train.csv not found in {train_dir}\")\n\n    print(f\"\\nSuccessfully loaded {len(image_paths)} images\")\n    print(f\"Number of classes: {len(class_to_idx)}\")\n    print(f\"\\nClasses:\")\n    for class_name, idx in sorted(class_to_idx.items(), key=lambda x: x[1]):\n        print(f\"  [{idx}] {class_name}\")\n\n    return image_paths, labels, class_to_idx\n\n# í•™ìŠµ ë°ì´í„° ë¡œë“œ\ntrain_csv_path = './data/train.csv'\nmeta_csv_path = './data/meta.csv'\n\nif os.path.exists(train_csv_path) and os.path.exists(meta_csv_path) and os.path.exists(CFG.train_dir):\n    train_paths, train_labels, class_to_idx = load_data_from_csv(train_csv_path, meta_csv_path, CFG.train_dir)\n\n    # CFG.num_classes ì—…ë°ì´íŠ¸\n    CFG.num_classes = len(class_to_idx)\nelse:\n    missing_files = []\n    if not os.path.exists(train_csv_path):\n        missing_files.append(train_csv_path)\n    if not os.path.exists(meta_csv_path):\n        missing_files.append(meta_csv_path)\n    if not os.path.exists(CFG.train_dir):\n        missing_files.append(CFG.train_dir)\n\n    print(f\"Error: Missing required files/directories:\")\n    for f in missing_files:\n        print(f\"  - {f}\")\n    print(\"\\nPlease upload your data or modify the paths in CFG.\")"
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\ndef visualize_random_samples(image_paths, labels, class_to_idx, num_samples=5):\n    \"\"\"\n    ë°ì´í„°ì…‹ì—ì„œ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ ì‹œê°í™”\n    \n    Args:\n        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n        labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸\n        class_to_idx: í´ë˜ìŠ¤ëª…-ì¸ë±ìŠ¤ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬\n        num_samples: ì‹œê°í™”í•  ìƒ˜í”Œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)\n    \"\"\"\n    # ì¸ë±ìŠ¤-í´ë˜ìŠ¤ëª… ë§¤í•‘\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    \n    # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ\n    random_indices = random.sample(range(len(image_paths)), min(num_samples, len(image_paths)))\n    \n    # í”Œë¡¯ ìƒì„±\n    fig, axes = plt.subplots(1, len(random_indices), figsize=(4 * len(random_indices), 4))\n    if len(random_indices) == 1:\n        axes = [axes]\n    \n    for idx, img_idx in enumerate(random_indices):\n        # ì´ë¯¸ì§€ ë¡œë“œ\n        img_path = image_paths[img_idx]\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # í´ë˜ìŠ¤ëª… ê°€ì ¸ì˜¤ê¸°\n        label_idx = labels[img_idx]\n        class_name = idx_to_class[label_idx]\n        \n        # ì‹œê°í™”\n        axes[idx].imshow(image)\n        axes[idx].set_title(f'Class: {class_name}\\n({os.path.basename(img_path)})', fontsize=10)\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ë°ì´í„°ê°€ ë¡œë“œëœ ê²½ìš° ëœë¤ ìƒ˜í”Œ ì‹œê°í™”\nif 'train_paths' in locals() and len(train_paths) > 0:\n    print(f\"\\n{'='*60}\")\n    print(\"ë°ì´í„°ì…‹ì—ì„œ ëœë¤ìœ¼ë¡œ 5ê°œ ìƒ˜í”Œ ì¶”ì¶œ (ì›ë³¸ ì´ë¯¸ì§€)\")\n    print(f\"{'='*60}\\n\")\n    visualize_random_samples(train_paths, train_labels, class_to_idx, num_samples=5)\n    \n    # í´ë˜ìŠ¤ë³„ ë¶„í¬ ì¶œë ¥\n    print(f\"\\n{'='*60}\")\n    print(\"í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ê°œìˆ˜:\")\n    print(f\"{'='*60}\")\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    class_counts = {}\n    for label in train_labels:\n        class_name = idx_to_class[label]\n        class_counts[class_name] = class_counts.get(class_name, 0) + 1\n    \n    for class_name in sorted(class_counts.keys()):\n        print(f\"  {class_name}: {class_counts[class_name]:4d} images\")\n    print(f\"{'='*60}\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8-1. ë°ì´í„°ì…‹ í™•ì¸ (ëœë¤ ìƒ˜í”Œ 5ê°œ ì‹œê°í™”)\n\në¡œë“œí•œ ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ê° í´ë˜ìŠ¤ì—ì„œ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 8-2. ì¶”ê°€ ë°ì´í„° ë¶„ì„ (EDA)\n\në°ì´í„°ì˜ íŠ¹ì„±ì„ ë” ìì„¸íˆ íŒŒì•…í•˜ê¸° ìœ„í•œ ì‹œê°í™”ì…ë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter\n\ndef visualize_class_distribution(labels, class_to_idx):\n    \"\"\"Visualize the distribution of classes in the dataset\"\"\"\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    class_names = [idx_to_class[label] for label in labels]\n    class_counts = Counter(class_names)\n    \n    classes = list(class_counts.keys())\n    counts = list(class_counts.values())\n    \n    plt.figure(figsize=(12, 6))\n    bars = plt.bar(classes, counts)\n    plt.xlabel('Class')\n    plt.ylabel('Number of Images')\n    plt.title('Class Distribution in Training Dataset')\n    plt.xticks(rotation=45, ha='right')\n    \n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{int(height)}',\n                ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nClass distribution:\")\n    for class_name, count in sorted(class_counts.items()):\n        print(f\"{class_name}: {count} images\")\n\ndef analyze_image_resolutions(image_paths, num_samples=None):\n    \"\"\"Analyze and visualize image resolution distribution\"\"\"\n    from PIL import Image\n    \n    if num_samples:\n        sample_paths = np.random.choice(image_paths, min(num_samples, len(image_paths)), replace=False)\n    else:\n        sample_paths = image_paths\n    \n    widths = []\n    heights = []\n    \n    for img_path in sample_paths:\n        try:\n            with Image.open(img_path) as img:\n                w, h = img.size\n                widths.append(w)\n                heights.append(h)\n        except Exception as e:\n            print(f\"Error loading {img_path}: {e}\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    axes[0].hist(widths, bins=30, edgecolor='black')\n    axes[0].set_xlabel('Width (pixels)')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Image Width Distribution')\n    axes[0].axvline(np.mean(widths), color='r', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n    axes[0].legend()\n    \n    axes[1].hist(heights, bins=30, edgecolor='black')\n    axes[1].set_xlabel('Height (pixels)')\n    axes[1].set_ylabel('Frequency')\n    axes[1].set_title('Image Height Distribution')\n    axes[1].axvline(np.mean(heights), color='r', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nResolution Statistics (from {len(widths)} images):\")\n    print(f\"Width  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.0f}\")\n    print(f\"Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.0f}\")\n\ndef visualize_class_grid(image_paths, labels, class_to_idx, samples_per_class=3):\n    \"\"\"Visualize random samples from each class in a grid\"\"\"\n    from PIL import Image\n    \n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n    \n    class_images = {class_name: [] for class_name in class_to_idx.keys()}\n    for img_path, label in zip(image_paths, labels):\n        class_name = idx_to_class[label]\n        class_images[class_name].append(img_path)\n    \n    num_classes = len(class_to_idx)\n    fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(samples_per_class * 3, num_classes * 3))\n    \n    if num_classes == 1:\n        axes = axes.reshape(1, -1)\n    \n    for idx, (class_name, img_paths) in enumerate(sorted(class_images.items())):\n        sample_paths = np.random.choice(img_paths, min(samples_per_class, len(img_paths)), replace=False)\n        \n        for col, img_path in enumerate(sample_paths):\n            try:\n                img = Image.open(img_path)\n                axes[idx, col].imshow(img)\n                axes[idx, col].axis('off')\n                if col == 0:\n                    axes[idx, col].set_ylabel(class_name, rotation=0, ha='right', va='center', fontsize=12)\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n        \n        for col in range(len(sample_paths), samples_per_class):\n            axes[idx, col].axis('off')\n    \n    plt.suptitle('Random Samples from Each Class', fontsize=16, y=0.995)\n    plt.tight_layout()\n    plt.show()\n\nif 'train_paths' in locals() and len(train_paths) > 0:\n    print(\"=\" * 50)\n    print(\"EXPLORATORY DATA ANALYSIS\")\n    print(\"=\" * 50)\n    \n    print(\"\\n1. Class Distribution Analysis\")\n    print(\"-\" * 50)\n    visualize_class_distribution(train_labels, class_to_idx)\n    \n    print(\"\\n2. Image Resolution Analysis\")\n    print(\"-\" * 50)\n    analyze_image_resolutions(train_paths, num_samples=500)\n    \n    print(\"\\n3. Visual Sample Grid\")\n    print(\"-\" * 50)\n    visualize_class_grid(train_paths, train_labels, class_to_idx, samples_per_class=3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation ë¶„í• \n",
    "if 'train_paths' in locals():\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_paths, train_labels, \n",
    "        test_size=CFG.val_ratio, \n",
    "        random_state=CFG.seed,\n",
    "        stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Train size: {len(train_paths)}\")\n",
    "    print(f\"Validation size: {len(val_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "if 'train_paths' in locals():\n",
    "    train_dataset = AlbumentationsDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = AlbumentationsDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=CFG.num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ì¦ê°• ê²°ê³¼ ì‹œê°í™” (ì„ íƒì‚¬í•­)\n",
    "\n",
    "ì¦ê°•ì´ ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_augmentations(dataset, idx=0, samples=5):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì˜ ì¦ê°• ê²°ê³¼ë¥¼ ì‹œê°í™”\n",
    "    \n",
    "    Args:\n",
    "        dataset: AlbumentationsDataset ê°ì²´\n",
    "        idx: ì‹œê°í™”í•  ì´ë¯¸ì§€ì˜ ì¸ë±ìŠ¤\n",
    "        samples: ìƒì„±í•  ì¦ê°• ìƒ˜í”Œ ìˆ˜\n",
    "    \"\"\"\n",
    "    # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "    img_path = dataset.image_paths[idx]\n",
    "    original_image = cv2.imread(img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # í”Œë¡¯ ìƒì„±\n",
    "    fig, axes = plt.subplots(1, samples + 1, figsize=(4 * (samples + 1), 4))\n",
    "    \n",
    "    # ì›ë³¸ ì´ë¯¸ì§€\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # ì¦ê°•ëœ ì´ë¯¸ì§€ë“¤\n",
    "    for i in range(samples):\n",
    "        augmented = dataset.transform(image=original_image)\n",
    "        aug_image = augmented['image']\n",
    "        \n",
    "        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜í•˜ê³  ì •ê·œí™” í•´ì œ\n",
    "        if isinstance(aug_image, torch.Tensor):\n",
    "            aug_image = aug_image.permute(1, 2, 0).numpy()\n",
    "            # ì •ê·œí™” í•´ì œ\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            aug_image = std * aug_image + mean\n",
    "            aug_image = np.clip(aug_image, 0, 1)\n",
    "        \n",
    "        axes[i + 1].imshow(aug_image)\n",
    "        axes[i + 1].set_title(f'Augmented {i+1}', fontsize=12)\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ì¦ê°• ì‹œê°í™” (ë°ì´í„°ê°€ ë¡œë“œëœ ê²½ìš°)\n",
    "if 'train_dataset' in locals() and len(train_dataset) > 0:\n",
    "    print(f\"Visualizing augmentations with level: {CFG.augmentation_level}\")\n",
    "    visualize_augmentations(train_dataset, idx=0, samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. WandB Run ì´ˆê¸°í™” (í•™ìŠµ ì‹œì‘ ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_experiment_number(project_name, prefix, entity=None):\n",
    "    \"\"\"WandBì—ì„œ ê¸°ì¡´ ì‹¤í—˜ë“¤ì„ í™•ì¸í•˜ê³  ë‹¤ìŒ ë²ˆí˜¸ë¥¼ ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  run ê°€ì ¸ì˜¤ê¸°\n",
    "        if entity:\n",
    "            runs = api.runs(f\"{entity}/{project_name}\")\n",
    "        else:\n",
    "            runs = api.runs(project_name)\n",
    "        \n",
    "        # prefixë¡œ ì‹œì‘í•˜ëŠ” runë“¤ì˜ ë²ˆí˜¸ ì¶”ì¶œ\n",
    "        numbers = []\n",
    "        for run in runs:\n",
    "            if run.name.startswith(prefix):\n",
    "                try:\n",
    "                    # 'prefix_123' í˜•íƒœì—ì„œ 123 ì¶”ì¶œ\n",
    "                    num = int(run.name.split('_')[-1])\n",
    "                    numbers.append(num)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # ê°€ì¥ í° ë²ˆí˜¸ + 1 ë°˜í™˜\n",
    "        next_num = max(numbers) + 1 if numbers else 1\n",
    "        return next_num\n",
    "    except:\n",
    "        # API ì ‘ê·¼ ì‹¤íŒ¨ì‹œ 001ë¶€í„° ì‹œì‘\n",
    "        return 1\n",
    "\n",
    "# WandB Run ì´ˆê¸°í™”\n",
    "if CFG.use_wandb:\n",
    "    # experiment_prefixê°€ Noneì´ë©´ ëª¨ë¸ëª… ì‚¬ìš© (ìë™)\n",
    "    if CFG.experiment_prefix is None:\n",
    "        actual_prefix = f\"{CFG.model_name}_alb_{CFG.augmentation_level}\"\n",
    "    else:\n",
    "        actual_prefix = CFG.experiment_prefix\n",
    "    \n",
    "    # ì‹¤í—˜ëª… ìë™ ìƒì„±\n",
    "    if CFG.experiment_name is None:\n",
    "        exp_num = get_next_experiment_number(\n",
    "            CFG.wandb_project, \n",
    "            actual_prefix,\n",
    "            CFG.wandb_entity\n",
    "        )\n",
    "        CFG.experiment_name = f\"{actual_prefix}_{exp_num:03d}\"\n",
    "    \n",
    "    run = wandb.init(\n",
    "        project=CFG.wandb_project,\n",
    "        entity=CFG.wandb_entity,\n",
    "        name=CFG.experiment_name,\n",
    "        config={\n",
    "            \"model_name\": CFG.model_name,\n",
    "            \"num_classes\": CFG.num_classes,\n",
    "            \"img_size\": CFG.img_size,\n",
    "            \"epochs\": CFG.epochs,\n",
    "            \"batch_size\": CFG.batch_size,\n",
    "            \"learning_rate\": CFG.learning_rate,\n",
    "            \"weight_decay\": CFG.weight_decay,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "            \"scheduler\": \"CosineAnnealingLR\",\n",
    "            \"val_ratio\": CFG.val_ratio,\n",
    "            \"seed\": CFG.seed,\n",
    "            \"augmentation\": \"albumentations\",\n",
    "            \"augmentation_level\": CFG.augmentation_level,\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"WandB Run initialized: {run.name}\")\n",
    "    print(f\"WandB URL: {run.url}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"WandB is disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=True):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        \n",
    "        # ëª¨ë¸ì˜ classifier ë¶€ë¶„ ìˆ˜ì •\n",
    "        if 'efficientnet' in model_name:\n",
    "            in_features = self.model.classifier.in_features\n",
    "            self.model.classifier = nn.Linear(in_features, num_classes)\n",
    "        elif 'resnet' in model_name:\n",
    "            in_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(in_features, num_classes)\n",
    "        elif 'vit' in model_name:\n",
    "            in_features = self.model.head.in_features\n",
    "            self.model.head = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = DocumentClassifier(\n",
    "    model_name=CFG.model_name, \n",
    "    num_classes=CFG.num_classes, \n",
    "    pretrained=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: {CFG.model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# WandBì— ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¡œê¹…\n",
    "if CFG.use_wandb:\n",
    "    wandb.watch(model, log='all', log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        batch_loss = running_loss / (batch_idx + 1)\n",
    "        batch_acc = 100. * correct / total\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': batch_loss,\n",
    "            'acc': batch_acc\n",
    "        })\n",
    "        \n",
    "        # WandB ë¡œê¹… (ë§¤ ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if CFG.use_wandb:\n",
    "            wandb.log({\n",
    "                'train/batch_loss': loss.item(),\n",
    "                'train/batch_acc': 100. * predicted.eq(labels).sum().item() / labels.size(0),\n",
    "                'train/step': epoch * len(train_loader) + batch_idx\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    # Macro F1 Score ê³„ì‚°\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ F1 Score ê³„ì‚°\n",
    "    per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
    "    \n",
    "    # Confusion Matrix ê³„ì‚°\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return epoch_loss, macro_f1, per_class_f1, cm, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "best_f1 = 0.0\npatience_counter = 0  # Early Stopping ì¹´ìš´í„°\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_f1': []\n}\n\n# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì €ì¥ ê²½ë¡œ ìƒì„±\nif CFG.save_to_drive:\n    os.makedirs(CFG.drive_model_dir, exist_ok=True)\n    print(f\"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {CFG.drive_model_dir}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Early Stopping: Patience={CFG.early_stopping_patience}, Min Delta={CFG.early_stopping_min_delta}\")\nprint(f\"{'='*60}\\n\")\n\nfor epoch in range(CFG.epochs):\n    print(f\"\\nEpoch {epoch+1}/{CFG.epochs}\")\n    print(\"-\" * 50)\n    \n    # í•™ìŠµ\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch)\n    \n    # ê²€ì¦\n    val_loss, val_f1, per_class_f1, cm, val_preds, val_labels = validate(model, val_loader, criterion, device)\n    \n    # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # ê²°ê³¼ ì €ì¥\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_f1'].append(val_f1)\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"Val Loss: {val_loss:.4f}, Val Macro F1: {val_f1:.4f}\")\n    print(f\"Learning Rate: {current_lr:.6f}\")\n    \n    # WandB ë¡œê¹… (ì—í­ë³„)\n    if CFG.use_wandb:\n        # ê¸°ë³¸ ë©”íŠ¸ë¦­\n        log_dict = {\n            'epoch': epoch + 1,\n            'train/epoch_loss': train_loss,\n            'train/epoch_acc': train_acc,\n            'val/loss': val_loss,\n            'val/macro_f1': val_f1,\n            'learning_rate': current_lr,\n            'early_stopping/patience_counter': patience_counter,\n        }\n        \n        # í´ë˜ìŠ¤ë³„ F1 Score\n        if 'class_to_idx' in locals():\n            idx_to_class = {v: k for k, v in class_to_idx.items()}\n            for idx, f1 in enumerate(per_class_f1):\n                class_name = idx_to_class.get(idx, f'class_{idx}')\n                log_dict[f'val/f1_{class_name}'] = f1\n        \n        # Confusion Matrix (5 ì—í­ë§ˆë‹¤)\n        if (epoch + 1) % 5 == 0:\n            log_dict['val/confusion_matrix'] = wandb.plot.confusion_matrix(\n                probs=None,\n                y_true=val_labels,\n                preds=val_preds,\n                class_names=[idx_to_class.get(i, f'class_{i}') for i in range(CFG.num_classes)] if 'idx_to_class' in locals() else None\n            )\n        \n        wandb.log(log_dict)\n    \n    # Early Stopping ì²´í¬ ë° ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n    if val_f1 > best_f1 + CFG.early_stopping_min_delta:\n        # ì„±ëŠ¥ ê°œì„ ë¨\n        best_f1 = val_f1\n        patience_counter = 0  # ì¹´ìš´í„° ë¦¬ì…‹\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_f1': best_f1,\n        }\n        \n        # ë¡œì»¬ì— ì €ì¥\n        torch.save(checkpoint, CFG.local_model_path)\n        print(f\"âœ“ Best model saved locally! (F1: {best_f1:.4f})\")\n        \n        # êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥\n        if CFG.save_to_drive:\n            # ì‹¤í—˜ëª…ì„ íŒŒì¼ëª…ì— í¬í•¨\n            if CFG.use_wandb and CFG.experiment_name:\n                drive_model_path = f\"{CFG.drive_model_dir}/{CFG.experiment_name}_f1_{best_f1:.4f}.pth\"\n            else:\n                drive_model_path = f\"{CFG.drive_model_dir}/best_model_alb_{CFG.augmentation_level}_f1_{best_f1:.4f}.pth\"\n            \n            torch.save(checkpoint, drive_model_path)\n            print(f\"âœ“ Best model saved to drive: {drive_model_path}\")\n        \n        # WandBì— ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n        if CFG.use_wandb:\n            artifact = wandb.Artifact(\n                name=f'model-{run.id}',\n                type='model',\n                description=f'Best model with F1: {best_f1:.4f}',\n                metadata={\n                    'epoch': epoch + 1,\n                    'val_f1': val_f1,\n                    'val_loss': val_loss,\n                    'augmentation_level': CFG.augmentation_level,\n                }\n            )\n            artifact.add_file(CFG.local_model_path)\n            wandb.log_artifact(artifact)\n    else:\n        # ì„±ëŠ¥ ê°œì„  ì—†ìŒ\n        patience_counter += 1\n        print(f\"âš  No improvement. Patience: {patience_counter}/{CFG.early_stopping_patience}\")\n        \n        # Early Stopping ì²´í¬\n        if patience_counter >= CFG.early_stopping_patience:\n            print(f\"\\n{'='*60}\")\n            print(f\"Early Stopping triggered at epoch {epoch+1}\")\n            print(f\"Best Validation Macro F1: {best_f1:.4f}\")\n            print(f\"{'='*60}\")\n            break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training completed!\")\nprint(f\"Best Validation Macro F1: {best_f1:.4f}\")\nprint(f\"Augmentation level: {CFG.augmentation_level}\")\nprint(f\"Total epochs: {epoch+1}\")\nif patience_counter >= CFG.early_stopping_patience:\n    print(f\"Stopped early due to no improvement for {CFG.early_stopping_patience} epochs\")\nprint(f\"{'='*50}\")\n\n# ìµœì¢… ëª¨ë¸ ê²½ë¡œ ì¶œë ¥\nprint(f\"\\nëª¨ë¸ ì €ì¥ ìœ„ì¹˜:\")\nprint(f\"  - ë¡œì»¬: {CFG.local_model_path}\")\nif CFG.save_to_drive:\n    print(f\"  - ë“œë¼ì´ë¸Œ: {CFG.drive_model_dir}/\")\n\n# WandB Run ì¢…ë£Œ\nif CFG.use_wandb:\n    wandb.finish()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss ê·¸ë˜í”„\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title(f'Training and Validation Loss (Albumentations {CFG.augmentation_level})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 Score ê·¸ë˜í”„\n",
    "axes[1].plot(history['val_f1'], label='Val Macro F1', color='orange')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Macro F1 Score')\n",
    "axes[1].set_title(f'Validation Macro F1 Score (Albumentations {CFG.augmentation_level})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Best model loaded (F1: {checkpoint['best_f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "def load_test_data(test_dir):\n",
    "    test_paths = []\n",
    "    for img_name in sorted(os.listdir(test_dir)):\n",
    "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            test_paths.append(os.path.join(test_dir, img_name))\n",
    "    return test_paths\n",
    "\n",
    "if os.path.exists(CFG.test_dir):\n",
    "    test_paths = load_test_data(CFG.test_dir)\n",
    "    print(f\"Total test images: {len(test_paths)}\")\n",
    "else:\n",
    "    print(f\"Warning: {CFG.test_dir} does not exist!\")\n",
    "    test_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±\n",
    "if test_paths:\n",
    "    test_dataset = AlbumentationsDataset(test_paths, labels=None, transform=val_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  í•¨ìˆ˜\n",
    "def predict(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader, desc='Predicting'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ì¶”ë¡  ì‹¤í–‰\n",
    "if test_paths:\n",
    "    predictions = predict(model, test_loader, device)\n",
    "    print(f\"Prediction completed: {len(predictions)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„± (í˜•ì‹ì€ ëŒ€íšŒ ê·œì •ì— ë§ê²Œ ìˆ˜ì •)\n",
    "if test_paths and predictions:\n",
    "    # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'image': [os.path.basename(path) for path in test_paths],\n",
    "        'label': [idx_to_class[pred] for pred in predictions]\n",
    "    })\n",
    "    \n",
    "    submission_filename = f'submission_alb_{CFG.augmentation_level}.csv'\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"\\nSubmission file saved: {submission_filename}\")\n",
    "    print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. ê³ ê¸‰ íŒ ë° ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´\n",
    "\n",
    "### ğŸ”¬ ì»¤ìŠ¤í…€ ì¦ê°• ë§Œë“¤ê¸°\n",
    "\n",
    "ê¸°ë³¸ ì œê³µë˜ëŠ” 3ê°€ì§€ ë ˆë²¨ ì™¸ì— ì§ì ‘ ì»¤ìŠ¤í…€ ì¦ê°•ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "# ì˜ˆì‹œ: ë¬¸ì„œ ìŠ¤ìº” íŠ¹í™” ì¦ê°•\n",
    "custom_transform = A.Compose([\n",
    "    A.Resize(CFG.img_size, CFG.img_size),\n",
    "    \n",
    "    # ìŠ¤ìº” ì‹œ ë°œìƒí•˜ëŠ” ë…¸ì´ì¦ˆ\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "        A.ISONoise(p=1.0),\n",
    "    ], p=0.3),\n",
    "    \n",
    "    # êµ¬ê²¨ì§„ ë¬¸ì„œ íš¨ê³¼\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n",
    "    \n",
    "    # íŒ©ìŠ¤/ë³µì‚¬ê¸° ë¸”ëŸ¬\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Test Time Augmentation (TTA)\n",
    "\n",
    "ì¶”ë¡  ì‹œì—ë„ ì¦ê°•ì„ ì ìš©í•˜ì—¬ **ì•™ìƒë¸” íš¨ê³¼**ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "def predict_with_tta(model, image, transforms, n_augmentations=5):\n",
    "    \"\"\"\n",
    "    TTAë¥¼ ì ìš©í•œ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "    ê°™ì€ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ë²ˆ ì¦ê°•í•˜ê³  ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ë” ì•ˆì •ì ì¸ ê²°ê³¼ íšë“\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_augmentations):\n",
    "            augmented = transforms(image=image)\n",
    "            img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
    "            output = model(img_tensor)\n",
    "            predictions.append(output.softmax(dim=1))\n",
    "    \n",
    "    # í‰ê·  ì˜ˆì¸¡\n",
    "    avg_prediction = torch.stack(predictions).mean(dim=0)\n",
    "    return avg_prediction.argmax(dim=1).item()\n",
    "```\n",
    "\n",
    "**TTA ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:**\n",
    "- ì¶”ë¡  ì‹œê°„ì´ n_augmentationsë°° ì¦ê°€\n",
    "- ì¼ë°˜ì ìœ¼ë¡œ 0.5-2% ì„±ëŠ¥ í–¥ìƒ\n",
    "- ìµœì¢… ì œì¶œ ì‹œì—ë§Œ ì‚¬ìš© ê¶Œì¥\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì¶”ê°€ ì•„ì´ë””ì–´\n",
    "\n",
    "#### 1. **ì¦ê°• íŒŒë¼ë¯¸í„° íŠœë‹**\n",
    "```python\n",
    "# WandB Sweepìœ¼ë¡œ ìµœì  ì¦ê°• ê°•ë„ ì°¾ê¸°\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'parameters': {\n",
    "        'augmentation_level': {\n",
    "            'values': ['light', 'medium', 'heavy']\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. **MixUp / CutMix**\n",
    "```python\n",
    "# Albumentationsì˜ MixUp transform\n",
    "A.MixUp(alpha=0.2, p=0.5)\n",
    "```\n",
    "\n",
    "#### 3. **í´ë˜ìŠ¤ë³„ ë§ì¶¤ ì¦ê°•**\n",
    "```python\n",
    "# ë¶ˆê· í˜• ë°ì´í„°ì…‹ì˜ ê²½ìš° ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë” ê°•í•œ ì¦ê°• ì ìš©\n",
    "if label in minority_classes:\n",
    "    transform = heavy_transform\n",
    "else:\n",
    "    transform = medium_transform\n",
    "```\n",
    "\n",
    "#### 4. **AutoAugment / RandAugment**\n",
    "```python\n",
    "# ìë™ìœ¼ë¡œ ìµœì  ì¦ê°• ì •ì±… í•™ìŠµ\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "# Albumentationsë„ AutoAugment ì§€ì›\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” ë””ë²„ê¹… íŒ\n",
    "\n",
    "**ì¦ê°•ì´ ë„ˆë¬´ ê°•í•´ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ë©´:**\n",
    "1. ì„¹ì…˜ 9ì˜ ì‹œê°í™”ë¡œ ì¦ê°• ê²°ê³¼ í™•ì¸\n",
    "2. `p` (í™•ë¥ ) íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "3. ë” ì•½í•œ ë ˆë²¨ë¡œ ë³€ê²½\n",
    "\n",
    "**í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ë‹¤ë©´:**\n",
    "1. Learning rate ê°ì†Œ\n",
    "2. Batch size ì¦ê°€\n",
    "3. ì¦ê°• ê°•ë„ ê°ì†Œ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Albumentations ê³µì‹ ë¬¸ì„œ](https://albumentations.ai/docs/)\n",
    "- [Albumentations ì˜ˆì œ ëª¨ìŒ](https://albumentations.ai/docs/examples/)\n",
    "- [ë¬¸ì„œ ì´ë¯¸ì§€ ì¦ê°• Best Practices](https://arxiv.org/abs/2106.08322)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}